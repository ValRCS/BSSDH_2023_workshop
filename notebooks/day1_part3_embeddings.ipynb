{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building embeddings from our corpus\n",
    "\n",
    "Now we have our corpus, we can build our embeddings.\n",
    "\n",
    "There are different types of embeddings. At our corpus we will work with word embeddings. However we could think of sentence embeddings, paragraph embeddings, document embeddings, even character or subword embeddings.\n",
    "\n",
    "Word embeddings are vector representations of words. They are useful for many NLP tasks, such as sentiment analysis, text classification, and machine translation. They are also useful for visualising the relationships between words. For example, we can use them to find words with similar meanings, or to find words that are often used together.\n",
    "\n",
    "## Why do we need embeddings?\n",
    "\n",
    "Computers and most machine learnign algorithms in particular work better with numerical data. However, words are not numbers. So we need to find a way to represent words as numbers.\n",
    "\n",
    "Large Language Models such as ChatGPT also use word embeddings to represent words. However, they are trained on huge amounts of data, and so are not always suitable for smaller projects. In this notebook, we will build our own word embeddings from our corpus.\n",
    "\n",
    "## Different types of embedddings\n",
    "\n",
    "There are different types of word embeddings as well, such as:\n",
    "\n",
    "- Word2Vec\n",
    "- GloVe\n",
    "- FastText\n",
    "- BERT\n",
    "\n",
    " We will  also use the gensim library to do this. Gensim is a library for topic modelling, document indexing and similarity retrieval with large corpora. It uses the word2vec algorithm to create vector representations of words, which can then be used to find words with similar meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
