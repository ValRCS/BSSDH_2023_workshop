{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Content Analysis - Setup \n",
    "\n",
    "![BSSDH](https://site-512948.mozfiles.com/files/512948/DHbaneris2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Text Content Analysis Workshop!\n",
    "\n",
    "### A few words about me\n",
    "\n",
    "Valdis Saulespurens \n",
    "\n",
    " * researcher at the National Library of Latvia, digital development department\n",
    " * 30+ years programming, 15 years with Python, 5 of those teaching Python\n",
    " * lecturer at Riga Technical University, Riga Business School\n",
    " * contact: valdis.saulespurens@lnb.lv - email, [Valdis on LinkedIn](https://www.linkedin.com/in/valdis-saulespurens), [ValRCS on Github](https://github.com/ValRCS)\n",
    "\n",
    "\n",
    "### A few words about the workshop\n",
    "\n",
    "* Two days - we meet on 25th and 26th of July of 2023 here at National Library of Latvia.\n",
    "#### Day 1 - 25th of July\n",
    "* Part 1 - 11:00 - 12:30 - Introduction to Text Content Analysis\n",
    "* Lunch Break 1 hour - 12:30 - 13:30\n",
    "* Part 2 - 13:30 - 15:00 - Cleaning, preprocessing and tokenization\n",
    "* Coffee Break 20 min \n",
    "* Part 3 - 15:20 - 16:50 - Creating embeddings \n",
    "\n",
    "#### Day 2 - 26th of July\n",
    "* Part 4 - 11:00 - 12:30 - Topic modeling\n",
    "* Lunch Break 1 hour - 12:30 - 13:30\n",
    "* Part 5 - 13:30 - 15:00 - Trend analysis, visualization and interpretation\n",
    "* Coffee Break 20 min\n",
    "* Part 6 - 15:20 - 16:50 - Your own work in class with the help of the instructor - required for the certificate/gaining credits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your computer setup\n",
    "\n",
    "This workshop will be using Python programming language and Jupyter Notebooks. Some minimal prerequisites are required to be able to run the notebooks in this repository.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "* You know a little bit about Python - refresher is provided in this repository\n",
    "* You have a Google account (gmail) and can use Google Colab\n",
    "\n",
    "For those new to Jupyter Notebooks - they are a way to combine text and code in a single document. You can run the code and see the results right in the notebook. You can also edit the code and run it again. This is a very convenient way to learn and experiment with Python. More on Jupyter Notebooks here : [Jupyter Notebooks](https://jupyter.org/)\n",
    "\n",
    "Jupyter Notebooks can be run locally on your computer or in the cloud. The primary/minimal option is using Google Colab - a cloud based Jupyter Notebook environment. You will need a Google account (gmail) to use Google Colab.\n",
    "\n",
    "I as an instructor will be using Visual Studio Code with Python extension and git support. This is a very powerful environment for Python development. You can install it on your computer and use it for this workshop. You will need to install Python and git on your computer - instructions were sent and are below. This is the preferred option for this workshop.\n",
    "\n",
    "### Test minimal prerequisites\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ValRCS/BSSDH_2023_workshop/blob/main/notebooks/test_python_setup.ipynb)\n",
    "\n",
    "You can run this same notebook in your own local environment if you have Python and git installed.\n",
    "\n",
    "\n",
    "### Practice Your Python Notebook skills (includes NumPy and Pandas library refresher)\n",
    "\n",
    "Here is a Python syntax refresher, optional if you have good working knowledge of Python.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ValRCS/BSSDH_2023_workshop/blob/main/notebooks/python_colab.ipynb)\n",
    "\n",
    "Again you can run this notebook in your own local environment if you have Python and git installed.\n",
    "\n",
    "### Local install instructions\n",
    "\n",
    "Detailed local install instructions can be found in INSTALL.md file in this repository. Link : [INSTALL.md](https://github.com/ValRCS/BSSDH_2023_workshop/blob/main/INSTALL.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Text Analysis\n",
    "\n",
    "### What is Text Analysis?\n",
    "\n",
    "Text analysis is the process of transforming unstructured text documents into structured data for further analysis. It is a form of data mining that is used to identify patterns and establish relationships between words in a text-based dataset. Text analysis is also known as text mining, text analytics, and data mining.\n",
    "\n",
    "### Why is Text Analysis Important in research and academia?\n",
    "\n",
    "Text analysis is important because it is a valuable method for extracting meaning from text-based data. It is used to quantify qualitative data, which is particularly helpful for research that involves collecting large amounts of unstructured data, such as customer feedback, open-ended survey responses, and social media comments.\n",
    "\n",
    "Text analysis is part of discourse analysis, which is the study of language use in texts and contexts. It is used to analyze the structure of written texts and is often used in the humanities and social sciences to analyze texts such as interview transcripts, news articles, and speeches.\n",
    "\n",
    "### Modeling Text Data after Preprocessing\n",
    "\n",
    "Text data is often modeled as numerical data after preprocessing. This is because most machine learning algorithms require numerical data as input. The most common way to model text data is to use a bag-of-words model, which represents each document as a vector of word counts. This is a simple and effective way to represent text data, but it does not capture the order of words in a document.\n",
    "We will be looking at some other ways to model text data in this workshop.\n",
    "\n",
    "## Pipeline - Plan of Attack\n",
    "\n",
    "### 1. Data Collection\n",
    "\n",
    "We will want to obtain data from some source. This could be a website, a database, or a file. We will be using a file for this workshop.\n",
    "However not every file is ready to be analyzed. We will need to clean the data and prepare it for analysis.\n",
    "\n",
    "Our first goal will be to read the data into a pandas dataframe. We will be using the `pandas` library for this. Pandas is a Python library that is used for data manipulation and analysis. It offers data structures and operations for manipulating numerical tables and time series. It also provides powerful data structures that are designed to make working with structured data fast, easy, and expressive.\n",
    "\n",
    "The process of importing data into a tabular format can be very easy or it can take quite some effort. We will work with medium sized datasets for this workshop. However, if you are working with truly large datasets, you may need to use a distributed computing framework such as Apache Spark to import the data. - this places huge demands on your hardware and is not recommended for beginners.\n",
    "Also Apache Spark is quite a bit slower when working with small datasets and few machines than regular Pandas based workflows.\n",
    "\n",
    "### 2. Data Preprocessing\n",
    "\n",
    "We will want to clean the data and prepare it for analysis. This includes removing punctuation, numbers, and other non-text characters. We will also want to remove stopwords, which are common words that do not add much meaning to a sentence, such as \"the\", \"and\", and \"a\". We will also want to remove words that appear too frequently or too infrequently in the dataset. This is known as removing words with high and low document frequency.\n",
    "\n",
    "### 3. Data Processing\n",
    "\n",
    "We will want to process the data in order to extract features from it. This includes tokenization, which is the process of splitting a text document into individual words. We will also want to stem and lemmatize the words in the dataset. Stemming is the process of reducing a word to its root form. Lemmatization is the process of reducing a word to its dictionary form. We will also want to remove words that are not nouns, verbs, adjectives, or adverbs. This is known as part-of-speech tagging.\n",
    "\n",
    "### 4. Creating Embeddings\n",
    "\n",
    "We will want to create embeddings from the data in order to represent the words in the dataset as vectors. This is known as word embedding. We will be using the `gensim` library for this. Gensim is a Python library for topic modeling, document indexing, and similarity retrieval with large corpora. It uses a fast implementation of the Word2Vec algorithm to learn vector representations of words.\n",
    "\n",
    "### 5. Modeling\n",
    "\n",
    "We will want to model the data in order to extract insights from it. This includes topic modeling, which is the process of discovering topics in a text-based dataset. We will might want to perform sentiment analysis, which is the process of determining whether a text document is positive, negative, or neutral. We will be using the `scikit-learn` library for this. Scikit-learn is a Python library for machine learning. It provides a range of supervised and unsupervised learning algorithms for classification, regression, and clustering.\n",
    "\n",
    "### 6. Visualization and Interpretation\n",
    "\n",
    "We will want to visualize the data in order to communicate our findings to others. This includes creating interactive visualizations using the Plotly library. Plotly is a Python library for creating interactive visualizations. It provides a range of tools for creating charts, maps, and graphs. We will also want to interpret the data in order to gain insights from it. This includes using the results of our analysis to make decisions about the data.\n",
    "\n",
    "For extra visualization of LDAModels we will be using pyLDAvis library - time permitting. It requires some extra setup and is not included in the Google Colab environment.\n",
    "\n",
    "### 7. Your own work in class  - required for the certificate/gaining credits\n",
    "\n",
    "In the last part of workshop you will be tasked with obtaining your own data and performing the same steps as in the workshop. You will be able to ask questions and get help from the instructor. This is required for the certificate/gaining credits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "### General Considerations and sources\n",
    "\n",
    "There are various sources of text data. Some of the most common sources are:\n",
    "\n",
    "* Web pages - see workshop on web scraping\n",
    "* Social media - generally need API access -- see issues with Twitter API\n",
    "* Books - see Project Gutenberg\n",
    "* News articles - see News API for organized access\n",
    "* Research papers - see ArXiv API\n",
    "* Wikipedia - see Wikipedia API\n",
    "* Blogs - see Blogger API or web scraping\n",
    "* Emails - see Enron Email Dataset as one example\n",
    "* Speeches - see American Presidency Project\n",
    "* Curated datasets - see Kaggle - note possible licensing issues\n",
    "* Dataset search - see [Google Dataset Search](https://datasetsearch.research.google.com/) - beware Killed By Google syndrome\n",
    "* US open data - see [data.gov](https://www.data.gov/)\n",
    "* European open data - see [European Data Portal](https://www.europeandataportal.eu/en)\n",
    "* Latvian open data - see [Latvian Open Data Portal](https://data.gov.lv/)\n",
    "* Your own data - see your own data you collected or have access to\n",
    "\n",
    "### Clarin - Reputable source of data\n",
    "\n",
    "Clarin is a European research infrastructure for language resources and technology. It is a networked federation of centres pooling their human and technical resources to create an infrastructure. The infrastructure consists of an interconnected network of repositories, service centres and knowledge centres, offering language resources (datasets) and natural language processing (NLP) tools and expertise. The infrastructure offers widespread access to language resources and advanced tools to support researchers in the humanities and social sciences, and beyond.\n",
    "\n",
    "[Clarin](https://www.clarin.eu/resource-families/historical-corpora)\n",
    "\n",
    "\n",
    "\n",
    "![Old Bailey](https://www.clarin.eu/sites/default/files/styles/large/public/media/showcases/Old_Bailey_Microcosm_edited_0.jpg?itok%253Dik2HYoSp)\n",
    "\n",
    "### Old Bailey Corpus\n",
    "\n",
    "The Old Bailey Corpus is a collection of aproximately 130 million words of text from the proceedings of the Old Bailey, a criminal court in London that operated from 1674 to 1913. The corpus is available for download from the Old Bailey Online website and other sources such as Clarin.\n",
    "\n",
    "Official Bailiff's website: [Old Bailey Online](https://www.oldbaileyonline.org/)\n",
    "\n",
    "Link at Clarin: [Old Bailey Corpus](https://www.clarin.eu/showcase/old-bailey-corpus-20-1720-1913)\n",
    "\n",
    "\n",
    "\n",
    "### Downloading the Data\n",
    "\n",
    "* Size: 134 Million Words\n",
    "* Annotation: detailed sociobiographical, pragmatic and textual annotation\n",
    "* Licence: CC-BY-NC-SA 4.0\n",
    "\n",
    "Note: Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License is a free license that allows you to share your work with others, as long as you credit the original author and indicate if changes were made. It also allows you to adapt the work and use it for non-commercial purposes, as long as you distribute it under the same license.\n",
    "\n",
    "More on Creative Commons Licenses: [Creative Commons](https://creativecommons.org/licenses/)\n",
    "\n",
    "Full size of the corpus is around 200MB, we will be using a selection of cases from each decade that is in the full corpus. This will be around 17MB of data uncompressed - around 3MB compressed.\n",
    "\n",
    "\n",
    "### Some considerations on downloading data\n",
    "\n",
    "* Data is available in various formats - we will be using XML format - this is a text based format that is supposed to be human readable, and computer parsable\n",
    "* Data is available in various sizes - we will be using a subset of the data - around 17MB uncompressed\n",
    "* We will be downloading the data from github repository - this is a very convenient way to share data and code as long as the data is not too large - github has a 100MB limit on file size without extra setup\n",
    "* if you are using local setup your data will already be in data folder - you could skip the download step but try to follow the rest of the steps\n",
    "* Google Colab does not provide automatic access to your local files - you will need to upload the data to Google Colab environment - this is a bit cumbersome but doable\n",
    "* We will be using a python library called `requests` to download the data - this is a very convenient way to download data from the internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Good Practices in creating notebooks\n",
    "\n",
    "* Use markdown cells to explain what you are doing and why\n",
    "* See Markdown cheatsheet at Github : [Markdown Cheatsheet](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)\n",
    "* Markdown headings can serve as table of contents for your notebook - use only one level 1 heading\n",
    "* Use comments in code cells to explain what you are doing and why\n",
    "* Use descriptive variable names - avoid single letter variable names\n",
    "* Use descriptive function names - avoid single letter function names\n",
    "* Use descriptive file names - avoid single letter file names\n",
    "* Use descriptive folder names - avoid single letter folder names - you get the idea by now :)\n",
    "* Use version control - git is the most popular version control system - we will be using it in this workshop\n",
    "* Note: git is not ideal for notebooks (JSON format) - it is better suited for pure code files - but it is better than nothing\n",
    "* Your notebook should be able to run all the way through without errors - this is not always possible but should be the goal\n",
    "* In other words your notebook should be able to be exported and run as standalone python file in ideal case - not always possible but should be the goal\n",
    "\n",
    "Finally, remember creating notebooks is a fusion of logical and creative process and you might not always know where you will end up. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]\n",
      "Pandas version: 2.0.3\n"
     ]
    }
   ],
   "source": [
    "## Finally we can get started with some real coding!\n",
    "\n",
    "## First we will want to import all the libraries need for this particular notebook\n",
    "\n",
    "## Note: When developing your notebook it might be okay to import libraries as you need them, but when you are done with your notebook you should move all your imports to the top of the notebook.\n",
    "\n",
    "## First we want to import standard libraries that come with python. These are libraries that are installed with python and you don't have to install them yourself.\n",
    "\n",
    "# we might need sys to get the version of python we are using and possibly other things\n",
    "import sys\n",
    "# print python version\n",
    "print(\"Python version:\", sys.version) # not really needed but good to know in case something goes wrong\n",
    "\n",
    "# we will want Path module from pathlib to work with paths\n",
    "from pathlib import Path # notice how we only import Path from pathlib and not the whole library\n",
    "\n",
    "# we will be forking with zipfiles so we will need zipfile\n",
    "import zipfile\n",
    "\n",
    "# we will be working with xml files so we will need xml\n",
    "import xml.etree.ElementTree as ET # so we imported xml but we only imported the ElementTree module from xml\n",
    "# we also renamed ElementTree to ET so we don't have to type out ElementTree every time we want to use it\n",
    "\n",
    "# we might need some regular expression magic\n",
    "import re\n",
    "\n",
    "# we might want to deal with some json data\n",
    "import json # also standard library\n",
    "\n",
    "# we might want to deal with some dates\n",
    "from datetime import datetime # also standard library\n",
    "# note how we imported datetime from datetime, this is because datetime is a module and a class in the datetime library\n",
    "# a bit of unfortunate naming but we can deal with it\n",
    "\n",
    "## External Libraries\n",
    "## These are libraries that are not installed with python and you have to install them yourself. You can install them with pip or conda. We will use pip for this class.\n",
    "## Note: You can also import libraries with an alias. This is useful when you want to use a library but don't want to type out the whole name every time you use it. For example, we can import pandas as pd. This will allow us to use pandas but we only have to type pd when we want to use it.\n",
    "\n",
    "# we will import tqdm for progress bars\n",
    "from tqdm import tqdm # tqdm is a library for progress bars\n",
    "# strictly speaking we don't need to import tqdm but it makes our lives easier and it's nice to have progress bars\n",
    "\n",
    "# we will want to deal with web requests so requests provides a nice interface for that\n",
    "import requests # requests is a library for making web requests GET, POST, PUT, DELETE, etc.\n",
    "# it supports many features like authentication, cookies, sessions, etc.\n",
    "# more on it in your web scraping workshop\n",
    "\n",
    "import pandas as pd # pandas is a library for data analysis, commonly used in data science, machine learning, and artificial intelligence\n",
    "# notice how we renamed pandas to pd so we don't have to type out pandas every time we want to use it, this is very common\n",
    "# NOTE: Use common conventions when importing libraries. For example, pandas is commonly imported as pd, \n",
    "# numpy is commonly imported as np, matplotlib.pyplot is commonly imported as plt, etc.\n",
    "\n",
    "# for external libraries they will often provide a way to check the version of the library\n",
    "print(\"Pandas version:\", pd.__version__) # this is the version of pandas we are using\n",
    "# sometimes some functions might not work in older versions of the library so it's good to know what version you are using\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://github.com/ValRCS/BSSDH_2023_workshop/raw/main/data/old_bailey_sample_1720_1913.zip\n"
     ]
    }
   ],
   "source": [
    "# our first step is to find out url where our data is located\n",
    "# we can find this out by looking at the source code of the webpage\n",
    "# in this case if you check Github repository of this project you will find that Github will \n",
    "# provide an option to download data raw\n",
    "# you could do this manually by left clickin raw button in \n",
    "# https://github.com/ValRCS/BSSDH_2023_workshop/blob/main/data/old_bailey_sample_1720_1913.zip\n",
    "# alternatively you can right click raw button and click copy link address\n",
    "url = \"https://github.com/ValRCS/BSSDH_2023_workshop/raw/main/data/old_bailey_sample_1720_1913.zip\"\n",
    "# note raw part of the url, this is important because it will allow us to download the data directly\n",
    "# in general there would be some way to figure out the url but it's not always easy\n",
    "# let's print our url to make sure it's correct\n",
    "print(\"URL:\", url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\n",
      "Temp folder exists: True\n"
     ]
    }
   ],
   "source": [
    "# now we want to download the data and copy into temp folder\n",
    "# first we will want to creata a temp folder if it does not exist\n",
    "# to keep notebooks on the same path we will use Path.cwd() to get the current working directory\n",
    "# print current working directory\n",
    "print(\"Current working directory:\", Path.cwd())\n",
    "# then we will use Path.mkdir() to create a folder\n",
    "# we will use exist_ok=True to make sure we don't get an error if the folder already exists\n",
    "Path(Path.cwd() / \"temp\").mkdir(exist_ok=True)\n",
    "# check that we created the folder\n",
    "print(\"Temp folder exists:\", Path(Path.cwd() / \"temp\").exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save file as: old_bailey_sample_1720_1913.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "old_bailey_sample_1720_1913.zip: 100%|██████████| 2.90M/2.90M [00:00<00:00, 7.87MB/s]\n"
     ]
    }
   ],
   "source": [
    "# finally we are ready to download the data\n",
    "# we will use requests to download the data\n",
    "# we will use requests.get() to make a GET request to the url\n",
    "# we will use stream=True to stream the data instead of downloading it all at once\n",
    "# we will use allow_redirects=True to allow redirects\n",
    "# we will use timeout=10 to timeout after 10 seconds\n",
    "\n",
    "# we will use with statement to open a file\n",
    "# we will use open() to open a file\n",
    "# we will use \"wb\" to open a file in binary mode\n",
    "# we will use Path() to create a path to our file\n",
    "# we will use Path.cwd() to get the current working directory   \n",
    "# we will use / to join paths\n",
    "# we will use \"temp\" to create a path to our file\n",
    "\n",
    "# we will use tqdm() to create a progress bar\n",
    "\n",
    "# we will use .iter_content() to iterate over the content of the response\n",
    "# we will use chunk_size=1024 to iterate over the content in chunks of 1024 bytes\n",
    "\n",
    "# we will use .write() to write the content to the file\n",
    "\n",
    "# we will use .close() to close the file when we are done\n",
    "\n",
    "# we will use .raise_for_status() to raise an exception if the status code is not 200\n",
    "\n",
    "# we will get file name from our url it is the last part of the url after the last /\n",
    "# we will use .split() to split the url by /\n",
    "file_name = url.split(\"/\")[-1]\n",
    "print(\"Will save file as:\", file_name)\n",
    "\n",
    "# now let's download the data - we are chunking data to support large files\n",
    "\n",
    "with open(Path(Path.cwd() / \"temp\" / file_name), \"wb\") as file: # note wb - write binary\n",
    "    # to speed up the download we will use stream=True\n",
    "    # also we are using chunk_size=1024 to download the data in chunks of 1024 bytes\n",
    "    # you can adjust chunk_size to your liking\n",
    "    with requests.get(url, stream=True, allow_redirects=True, timeout=10) as response:\n",
    "        with tqdm(total=int(response.headers.get(\"content-length\", 0)), unit=\"B\", unit_scale=True, desc=file_name) as progress:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                file.write(chunk)\n",
    "                progress.update(len(chunk))\n",
    "        # file.close() # not required because of with statement\n",
    "        response.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 2900720\n"
     ]
    }
   ],
   "source": [
    "# now those on local machine can check that the data was downloaded to temp folder and is identical to same file in data folder\n",
    "# you can also check that the file size is the same\n",
    "# print file size\n",
    "print(\"File size:\", Path(Path.cwd() / \"temp\" / file_name).stat().st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original file size: 2900720\n"
     ]
    }
   ],
   "source": [
    "# print file size of original file - will not work in Colab!\n",
    "print(\"Original file size:\", Path(Path.cwd().parent / \"data\" / file_name).stat().st_size)\n",
    "## same file size is generally a good sign that the files are identical - but not always"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD5: 66c094035ceee02b306d110adf7cf658\n"
     ]
    }
   ],
   "source": [
    "# for more hardcore people you can use checksums to check that the files are identical\n",
    "# you can use md5, sha1, sha256, etc.\n",
    "# you can use hashlib to calculate checksums\n",
    "import hashlib\n",
    "# we will use md5\n",
    "md5 = hashlib.md5()\n",
    "# we will use .read_bytes() to read the file as bytes\n",
    "# we will use .update() to update the md5 hash\n",
    "md5.update(Path(Path.cwd() / \"temp\" / file_name).read_bytes())\n",
    "# we will use .hexdigest() to get the md5 hash as a string\n",
    "print(\"MD5:\", md5.hexdigest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# advantage of above approach is that you can check that the file is identical to the original file without having two copies of the file\n",
    "# you can also check that the file is identical to the original file without having to download the original file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are going to unzip c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\old_bailey_sample_1720_1913.zip\n",
      "We are going to unzip to c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\n"
     ]
    }
   ],
   "source": [
    "# So now we have downloaded the data and we can start working with it\n",
    "# first we will want to unzip the data\n",
    "# the simplest would be to use zipfile and just extract all of the files in original zip file keeping folder structure\n",
    "\n",
    "# let's do that for now\n",
    "\n",
    "# first we will want to create a path to our zip file\n",
    "zip_path = Path(Path.cwd() / \"temp\" / file_name)\n",
    "print(\"We are going to unzip\", zip_path) # the absolute path will be differnt on each machine!\n",
    "\n",
    "# unzip folder will be same temp folder\n",
    "unzip_folder = Path(Path.cwd() / \"temp\")\n",
    "print(\"We are going to unzip to\", unzip_folder) # the absolute path will be differnt on each machine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's unzip our zip file\n",
    "# we will use zipfile.ZipFile() to create a ZipFile object\n",
    "\n",
    "# we will use .extractall() to extract all files from the zip file to the unzip folder\n",
    "\n",
    "with zipfile.ZipFile(zip_path) as zip_file:\n",
    "    zip_file.extractall(unzip_folder) # extract all files to unzip folder keeping folder structure\n",
    "\n",
    "# there are other recipes for working with truly large zip files where you might not want to unzip the whole file at once\n",
    "# but for now we will keep it simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\LICENSE.txt\n",
      "Directory: c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\n",
      "File: c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\old_bailey_sample_1720_1913.zip\n",
      "File: c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\README.html\n"
     ]
    }
   ],
   "source": [
    "# we can see that there was some file structure in the zip file\n",
    "# we could use our file explorer or we could list the directory in temp\n",
    "# we will use Path.iterdir() to iterate over the files in the temp folder and print them\n",
    "for file in Path(unzip_folder).iterdir():\n",
    "    # if file is a directory we will print it as a directory\n",
    "    if file.is_dir():\n",
    "        print(\"Directory:\", file)\n",
    "    # if file is a file we will print it as a file\n",
    "    elif file.is_file():\n",
    "        print(\"File:\", file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 24\n",
      "First 10 files:\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\LICENSE.txt\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\old_bailey_sample_1720_1913.zip\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\README.html\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17200427.xml\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17310428.xml\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17420428.xml\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17540116.xml\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17620224.xml\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17731020.xml\n"
     ]
    }
   ],
   "source": [
    "# we can use Path.rglob() to recursively iterate over all files in the temp folder\n",
    "# we will use ** to recursively iterate over all files in the temp folder\n",
    "# we will use * to iterate over all files in the temp folder\n",
    "\n",
    "all_files = [file for file in Path(unzip_folder).rglob(\"*\")]\n",
    "print(\"Number of files:\", len(all_files))\n",
    "# print first 10 files\n",
    "print(\"First 10 files:\", *all_files[:10], sep=\"\\n\") # i unrolled the first 10 files with * and separated them with new line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of xml files: 20\n",
      "First 10 xml files:\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17200427.xml\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17310428.xml\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17420428.xml\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17540116.xml\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17620224.xml\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17731020.xml\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17841210.xml\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17961026.xml\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-18020217.xml\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-18140112.xml\n"
     ]
    }
   ],
   "source": [
    "# now we can see that we have a lot of files\n",
    "# how about only getting xml files because those are in fact what we want\n",
    "# let's use list comprehension to get only xml files\n",
    "xml_files = [file for file in Path(unzip_folder).rglob(\"*.xml\")]\n",
    "print(\"Number of xml files:\", len(xml_files))\n",
    "# print first 10 xml files\n",
    "print(\"First 10 xml files:\", *xml_files[:10], sep=\"\\n\") # i unrolled the first 10 files with * and separated them with new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 xml files sorted by name:\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17200427.xml\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17310428.xml\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17420428.xml\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17540116.xml\n",
      "c:\\Users\\Valdis\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17620224.xml\n"
     ]
    }
   ],
   "source": [
    "## we could provide various ordering of files, let's sort by file name since that is the simplest\n",
    "xml_files.sort(key=lambda x: x.name)\n",
    "print(\"First 5 xml files sorted by name:\", *xml_files[:5], sep=\"\\n\") # i unrolled the first 10 files with * and separated them with new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root tag: TEI.2\n"
     ]
    }
   ],
   "source": [
    "# looks like the first file by name is indeed the oldest file from 1720\n",
    "# let's load it in and see what we have\n",
    "# we will use xml.etree.ElementTree.parse() to parse the xml file\n",
    "# we will use .getroot() to get the root element of the xml file\n",
    "# we will use .tag to get the tag of the root element\n",
    "# we will use .attrib to get the attributes of the root element\n",
    "\n",
    "# let's get started\n",
    "\n",
    "tree = ET.parse(xml_files[0]) # parse the xml file\n",
    "root = tree.getroot() # get the root element\n",
    "print(\"Root tag:\", root.tag) # print the tag of the root element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEI - Text Encoding Initiative\n",
    "\n",
    "What is TEI?\n",
    "\n",
    "TEI - Text Encoding Initiative is a consortium which collectively develops and maintains a standard for the representation of texts in digital form. Its chief deliverable is a set of Guidelines which specify encoding methods for machine-readable texts, chiefly in the humanities, social sciences and linguistics. Since 1994, the TEI Guidelines have been widely used by libraries, museums, publishers, and individual scholars to present texts for online research, teaching, and preservation. In addition to the Guidelines themselves, the Consortium provides a variety of resources and training events for learning TEI, information on projects using the TEI, a bibliography of TEI-related publications, and software developed for or adapted to the TEI.\n",
    "\n",
    "### TEI and XML\n",
    "\n",
    "![TEI](https://tei-c.org/release/doc/tei-p5-doc/en/html/Images/banner.jpg)\n",
    "\n",
    "[TEI](https://tei-c.org/) is a standard for encoding text in XML. \n",
    "\n",
    "XML is a markup language that is used to describe data. XML is extremely flexible which is both a strength and a weakness. It is a strength because it allows you to create your own tags and attributes. It is a weakness because it can be difficult to understand and maintain.\n",
    "\n",
    "### XML Tutorial\n",
    "\n",
    "XML resembles HTMl in some ways as they share a common ancestor. XML is a markup language that is used to describe data. \n",
    "\n",
    "A basic tutorial on XML can be found here: [XML Tutorial](https://www.w3schools.com/xml/) - W3Schools is not officially affiliated with W3C. It used to be less reputable but has improved over the years.\n",
    "\n",
    "For additional complexity you can use namespaces to create your own tags and attributes. This is a way to avoid name collisions with other XML tags and attributes. This can add additional complexity to your code.\n",
    "\n",
    "TEI provides its own standart XML tags:\n",
    "See TEI documentation: [TEI](https://tei-c.org/release/doc/tei-p5-doc/en/html/index.html)\n",
    "\n",
    "A good starting point for sample document might here: [Default Document](https://tei-c.org/release/doc/tei-p5-doc/en/html/DS.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use any standart text editor to open the xml file and see what it looks like\n",
    "# I will be using this very same Visual Studio Code\n",
    "\n",
    "# upon inspection looks like I will be interested in div1 tags of type trialAccount\n",
    "# our next task will be to extract all the trialAccount div1 tags and save plain text of those div1 tags\n",
    "# we will also want to have some metadata about the trialAccount div1 tags such as year, trial number, and trial date if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trialAccount div1 tags: 75\n"
     ]
    }
   ],
   "source": [
    "# let's get a list of all div1 tags of type trialAccount\n",
    "# we will use .findall() to find all div1 tags\n",
    "# we will use .iter() to iterate over all div1 tags\n",
    "# we will use .attrib to get the attributes of the div1 tag\n",
    "# we will use .get() to get the value of the attribute\n",
    "# we will use .text to get the text of the div1 tag\n",
    "\n",
    "# we will use list comprehension to get all div1 tags of type trialAccount\n",
    "trial_accounts = [div1 for div1 in root.findall(\".//div1[@type='trialAccount']\")] \n",
    "# note we are using .//div1[@type='trialAccount'] to find all div1 tags of type trialAccount\n",
    "# this is using xpath syntax\n",
    "# more about xpath here: https://www.w3schools.com/xml/xpath_intro.asp\n",
    "print(\"Number of trialAccount div1 tags:\", len(trial_accounts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trialAccount div1 tags: 75\n"
     ]
    }
   ],
   "source": [
    "# if xpath is not your thing you can use .iter() to iterate over all div1 tags and check if they have type attribute and if it's value is trialAccount\n",
    "# we will use .iter() to iterate over all div1 tags\n",
    "# we will use .attrib to get the attributes of the div1 tag\n",
    "# we will use .get() to get the value of the attribute\n",
    "# we will use .text to get the text of the div1 tag\n",
    "\n",
    "# we will use a simple loop\n",
    "trial_accounts_too = []\n",
    "for div1 in root.iter(\"div1\"): # iterate over all div1 tags\n",
    "    if \"type\" in div1.attrib: # check if div1 tag has type attribute\n",
    "        if div1.attrib[\"type\"] == \"trialAccount\": # check if div1 tag has type attribute with value trialAccount\n",
    "            trial_accounts_too.append(div1) # if it does append it to trial_accounts_too\n",
    "print(\"Number of trialAccount div1 tags:\", len(trial_accounts_too))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
