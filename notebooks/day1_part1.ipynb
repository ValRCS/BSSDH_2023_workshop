{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Content Analysis - Setup \n",
    "\n",
    "![BSSDH](https://site-512948.mozfiles.com/files/512948/DHbaneris2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Text Content Analysis Workshop!\n",
    "\n",
    "### A few words about me\n",
    "\n",
    "Valdis Saulespurens \n",
    "\n",
    " * researcher at the National Library of Latvia, digital development department\n",
    " * 30+ years programming, 15 years with Python, 5 of those teaching Python\n",
    " * lecturer at Riga Technical University, Riga Business School\n",
    " * contact: valdis.saulespurens@lnb.lv - email, [Valdis on LinkedIn](https://www.linkedin.com/in/valdis-saulespurens), [ValRCS on Github](https://github.com/ValRCS)\n",
    "\n",
    "\n",
    "### A few words about the workshop\n",
    "\n",
    "* Two days - we meet on 25th and 26th of July of 2023 here at National Library of Latvia.\n",
    "#### Day 1 - 25th of July\n",
    "* Part 1 - 11:00 - 12:30 - Introduction to Text Content Analysis\n",
    "* Lunch Break 1 hour - 12:30 - 13:30\n",
    "* Part 2 - 13:30 - 15:00 - Cleaning, preprocessing and tokenization\n",
    "* Coffee Break 20 min \n",
    "* Part 3 - 15:20 - 16:50 - Creating embeddings \n",
    "\n",
    "#### Day 2 - 26th of July\n",
    "* Part 4 - 11:00 - 12:30 - Topic modeling\n",
    "* Lunch Break 1 hour - 12:30 - 13:30\n",
    "* Part 5 - 13:30 - 15:00 - Trend analysis, visualization and interpretation\n",
    "* Coffee Break 20 min\n",
    "* Part 6 - 15:20 - 16:50 - Your own work in class with the help of the instructor - required for the certificate/gaining credits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your computer setup\n",
    "\n",
    "This workshop will be using Python programming language and Jupyter Notebooks. Some minimal prerequisites are required to be able to run the notebooks in this repository.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "* You know a little bit about Python - refresher is provided in this repository\n",
    "* You have a Google account (gmail) and can use Google Colab\n",
    "\n",
    "For those new to Jupyter Notebooks - they are a way to combine text and code in a single document. You can run the code and see the results right in the notebook. You can also edit the code and run it again. This is a very convenient way to learn and experiment with Python. More on Jupyter Notebooks here : [Jupyter Notebooks](https://jupyter.org/)\n",
    "\n",
    "Jupyter Notebooks can be run locally on your computer or in the cloud. The primary/minimal option is using Google Colab - a cloud based Jupyter Notebook environment. You will need a Google account (gmail) to use Google Colab.\n",
    "\n",
    "I as an instructor will be using Visual Studio Code with Python extension and git support. This is a very powerful environment for Python development. You can install it on your computer and use it for this workshop. You will need to install Python and git on your computer - instructions were sent and are below. This is the preferred option for this workshop.\n",
    "\n",
    "### Test minimal prerequisites\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ValRCS/BSSDH_2023_workshop/blob/main/notebooks/test_python_setup.ipynb)\n",
    "\n",
    "You can run this same notebook in your own local environment if you have Python and git installed.\n",
    "\n",
    "\n",
    "### Practice Your Python Notebook skills (includes NumPy and Pandas library refresher)\n",
    "\n",
    "Here is a Python syntax refresher, optional if you have good working knowledge of Python.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ValRCS/BSSDH_2023_workshop/blob/main/notebooks/python_colab.ipynb)\n",
    "\n",
    "Again you can run this notebook in your own local environment if you have Python and git installed.\n",
    "\n",
    "### Local install instructions\n",
    "\n",
    "Detailed local install instructions can be found in INSTALL.md file in this repository. Link : [INSTALL.md](https://github.com/ValRCS/BSSDH_2023_workshop/blob/main/INSTALL.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Text Analysis\n",
    "\n",
    "### What is Text Analysis?\n",
    "\n",
    "Text analysis is the process of transforming unstructured text documents into structured data for further analysis. It is a form of data mining that is used to identify patterns and establish relationships between words in a text-based dataset. Text analysis is also known as text mining, text analytics, and data mining.\n",
    "\n",
    "### Why is Text Analysis Important in research and academia?\n",
    "\n",
    "Text analysis is important because it is a valuable method for extracting meaning from text-based data. It is used to quantify qualitative data, which is particularly helpful for research that involves collecting large amounts of unstructured data, such as customer feedback, open-ended survey responses, and social media comments.\n",
    "\n",
    "Text analysis is part of discourse analysis, which is the study of language use in texts and contexts. It is used to analyze the structure of written texts and is often used in the humanities and social sciences to analyze texts such as interview transcripts, news articles, and speeches.\n",
    "\n",
    "### Modeling Text Data after Preprocessing\n",
    "\n",
    "Text data is often modeled as numerical data after preprocessing. This is because most machine learning algorithms require numerical data as input. The most common way to model text data is to use a bag-of-words model, which represents each document as a vector of word counts. This is a simple and effective way to represent text data, but it does not capture the order of words in a document.\n",
    "We will be looking at some other ways to model text data in this workshop.\n",
    "\n",
    "## Pipeline - Plan of Attack\n",
    "\n",
    "### 1. Data Collection\n",
    "\n",
    "We will want to obtain data from some source. This could be a website, a database, or a file. We will be using a file for this workshop.\n",
    "However not every file is ready to be analyzed. We will need to clean the data and prepare it for analysis.\n",
    "\n",
    "Our first goal will be to read the data into a pandas dataframe. We will be using the `pandas` library for this. Pandas is a Python library that is used for data manipulation and analysis. It offers data structures and operations for manipulating numerical tables and time series. It also provides powerful data structures that are designed to make working with structured data fast, easy, and expressive.\n",
    "\n",
    "The process of importing data into a tabular format can be very easy or it can take quite some effort. We will work with medium sized datasets for this workshop. However, if you are working with truly large datasets, you may need to use a distributed computing framework such as Apache Spark to import the data. - this places huge demands on your hardware and is not recommended for beginners.\n",
    "Also Apache Spark is quite a bit slower when working with small datasets and few machines than regular Pandas based workflows.\n",
    "\n",
    "### 2. Data Preprocessing\n",
    "\n",
    "We will want to clean the data and prepare it for analysis. This includes removing punctuation, numbers, and other non-text characters. We will also want to remove stopwords, which are common words that do not add much meaning to a sentence, such as \"the\", \"and\", and \"a\". We will also want to remove words that appear too frequently or too infrequently in the dataset. This is known as removing words with high and low document frequency.\n",
    "\n",
    "### 3. Data Processing\n",
    "\n",
    "We will want to process the data in order to extract features from it. This includes tokenization, which is the process of splitting a text document into individual words. We will also want to stem and lemmatize the words in the dataset. Stemming is the process of reducing a word to its root form. Lemmatization is the process of reducing a word to its dictionary form. We will also want to remove words that are not nouns, verbs, adjectives, or adverbs. This is known as part-of-speech tagging.\n",
    "\n",
    "### 4. Creating Embeddings\n",
    "\n",
    "We will want to create embeddings from the data in order to represent the words in the dataset as vectors. This is known as word embedding. We will be using the `gensim` library for this. Gensim is a Python library for topic modeling, document indexing, and similarity retrieval with large corpora. It uses a fast implementation of the Word2Vec algorithm to learn vector representations of words.\n",
    "\n",
    "### 5. Modeling\n",
    "\n",
    "We will want to model the data in order to extract insights from it. This includes topic modeling, which is the process of discovering topics in a text-based dataset. We will might want to perform sentiment analysis, which is the process of determining whether a text document is positive, negative, or neutral. We will be using the `scikit-learn` library for this. Scikit-learn is a Python library for machine learning. It provides a range of supervised and unsupervised learning algorithms for classification, regression, and clustering.\n",
    "\n",
    "### 6. Visualization and Interpretation\n",
    "\n",
    "We will want to visualize the data in order to communicate our findings to others. This includes creating interactive visualizations using the Plotly library. Plotly is a Python library for creating interactive visualizations. It provides a range of tools for creating charts, maps, and graphs. We will also want to interpret the data in order to gain insights from it. This includes using the results of our analysis to make decisions about the data.\n",
    "\n",
    "For extra visualization of LDAModels we will be using pyLDAvis library - time permitting. It requires some extra setup and is not included in the Google Colab environment.\n",
    "\n",
    "### 7. Your own work in class  - required for the certificate/gaining credits\n",
    "\n",
    "In the last part of workshop you will be tasked with obtaining your own data and performing the same steps as in the workshop. You will be able to ask questions and get help from the instructor. This is required for the certificate/gaining credits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "### General Considerations and sources\n",
    "\n",
    "There are various sources of text data. Some of the most common sources are:\n",
    "\n",
    "* Web pages - see workshop on web scraping\n",
    "* Social media - generally need API access -- see issues with Twitter API\n",
    "* Books - see Project Gutenberg\n",
    "* News articles - see News API for organized access\n",
    "* Research papers - see ArXiv API\n",
    "* Wikipedia - see Wikipedia API\n",
    "* Blogs - see Blogger API or web scraping\n",
    "* Emails - see Enron Email Dataset as one example\n",
    "* Speeches - see American Presidency Project\n",
    "* Curated datasets - see Kaggle - note possible licensing issues\n",
    "* Dataset search - see [Google Dataset Search](https://datasetsearch.research.google.com/) - beware Killed By Google syndrome\n",
    "* US open data - see [data.gov](https://www.data.gov/)\n",
    "* European open data - see [European Data Portal](https://www.europeandataportal.eu/en)\n",
    "* Latvian open data - see [Latvian Open Data Portal](https://data.gov.lv/)\n",
    "* Your own data - see your own data you collected or have access to\n",
    "\n",
    "### Clarin - Reputable source of data\n",
    "\n",
    "Clarin is a European research infrastructure for language resources and technology. It is a networked federation of centres pooling their human and technical resources to create an infrastructure. The infrastructure consists of an interconnected network of repositories, service centres and knowledge centres, offering language resources (datasets) and natural language processing (NLP) tools and expertise. The infrastructure offers widespread access to language resources and advanced tools to support researchers in the humanities and social sciences, and beyond.\n",
    "\n",
    "[Clarin](https://www.clarin.eu/resource-families/historical-corpora)\n",
    "\n",
    "\n",
    "\n",
    "![Old Bailey](https://www.clarin.eu/sites/default/files/styles/large/public/media/showcases/Old_Bailey_Microcosm_edited_0.jpg?itok%253Dik2HYoSp)\n",
    "\n",
    "### Old Bailey Corpus\n",
    "\n",
    "The Old Bailey Corpus is a collection of aproximately 130 million words of text from the proceedings of the Old Bailey, a criminal court in London that operated from 1674 to 1913. The corpus is available for download from the Old Bailey Online website and other sources such as Clarin.\n",
    "\n",
    "Official Bailiff's website: [Old Bailey Online](https://www.oldbaileyonline.org/)\n",
    "\n",
    "Link at Clarin: [Old Bailey Corpus](https://www.clarin.eu/showcase/old-bailey-corpus-20-1720-1913)\n",
    "\n",
    "\n",
    "\n",
    "### Downloading the Data\n",
    "\n",
    "* Size: 134 Million Words\n",
    "* Annotation: detailed sociobiographical, pragmatic and textual annotation\n",
    "* Licence: CC-BY-NC-SA 4.0\n",
    "\n",
    "Note: Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License is a free license that allows you to share your work with others, as long as you credit the original author and indicate if changes were made. It also allows you to adapt the work and use it for non-commercial purposes, as long as you distribute it under the same license.\n",
    "\n",
    "More on Creative Commons Licenses: [Creative Commons](https://creativecommons.org/licenses/)\n",
    "\n",
    "Full size of the corpus is around 200MB, we will be using a selection of cases from each decade that is in the full corpus. This will be around 17MB of data uncompressed - around 3MB compressed.\n",
    "\n",
    "\n",
    "### Some considerations on downloading data\n",
    "\n",
    "* Data is available in various formats - we will be using XML format - this is a text based format that is supposed to be human readable, and computer parsable\n",
    "* Data is available in various sizes - we will be using a subset of the data - around 17MB uncompressed\n",
    "* We will be downloading the data from github repository - this is a very convenient way to share data and code as long as the data is not too large - github has a 100MB limit on file size without extra setup\n",
    "* if you are using local setup your data will already be in data folder - you could skip the download step but try to follow the rest of the steps\n",
    "* Google Colab does not provide automatic access to your local files - you will need to upload the data to Google Colab environment - this is a bit cumbersome but doable\n",
    "* We will be using a python library called `requests` to download the data - this is a very convenient way to download data from the internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Good Practices in creating notebooks\n",
    "\n",
    "* Use markdown cells to explain what you are doing and why\n",
    "* See Markdown cheatsheet at Github : [Markdown Cheatsheet](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)\n",
    "* Markdown headings can serve as table of contents for your notebook - use only one level 1 heading\n",
    "* Use comments in code cells to explain what you are doing and why\n",
    "* Use descriptive variable names - avoid single letter variable names\n",
    "* Use descriptive function names - avoid single letter function names\n",
    "* Use descriptive file names - avoid single letter file names\n",
    "* Use descriptive folder names - avoid single letter folder names - you get the idea by now :)\n",
    "* Use version control - git is the most popular version control system - we will be using it in this workshop\n",
    "* Note: git is not ideal for notebooks (JSON format) - it is better suited for pure code files - but it is better than nothing\n",
    "* Your notebook should be able to run all the way through without errors - this is not always possible but should be the goal\n",
    "* In other words your notebook should be able to be exported and run as standalone python file in ideal case - not always possible but should be the goal\n",
    "\n",
    "Finally, remember creating notebooks is a fusion of logical and creative process and you might not always know where you will end up. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]\n",
      "Pandas version: 2.0.3\n"
     ]
    }
   ],
   "source": [
    "## Finally we can get started with some real coding!\n",
    "\n",
    "## First we will want to import all the libraries need for this particular notebook\n",
    "\n",
    "## Note: When developing your notebook it might be okay to import libraries as you need them, but when you are done with your notebook you should move all your imports to the top of the notebook.\n",
    "\n",
    "## First we want to import standard libraries that come with python. These are libraries that are installed with python and you don't have to install them yourself.\n",
    "\n",
    "# we might need sys to get the version of python we are using and possibly other things\n",
    "import sys\n",
    "# print python version\n",
    "print(\"Python version:\", sys.version) # not really needed but good to know in case something goes wrong\n",
    "\n",
    "# we will want Path module from pathlib to work with paths\n",
    "from pathlib import Path # notice how we only import Path from pathlib and not the whole library\n",
    "\n",
    "# we will be forking with zipfiles so we will need zipfile\n",
    "import zipfile\n",
    "\n",
    "# we will be working with xml files so we will need xml\n",
    "import xml.etree.ElementTree as ET # so we imported xml but we only imported the ElementTree module from xml\n",
    "# we also renamed ElementTree to ET so we don't have to type out ElementTree every time we want to use it\n",
    "\n",
    "# we might need some regular expression magic\n",
    "import re\n",
    "\n",
    "# we might want to deal with some json data\n",
    "import json # also standard library\n",
    "\n",
    "# we might want to deal with some dates\n",
    "from datetime import datetime # also standard library\n",
    "# note how we imported datetime from datetime, this is because datetime is a module and a class in the datetime library\n",
    "# a bit of unfortunate naming but we can deal with it\n",
    "\n",
    "## External Libraries\n",
    "## These are libraries that are not installed with python and you have to install them yourself. You can install them with pip or conda. We will use pip for this class.\n",
    "## Note: You can also import libraries with an alias. This is useful when you want to use a library but don't want to type out the whole name every time you use it. For example, we can import pandas as pd. This will allow us to use pandas but we only have to type pd when we want to use it.\n",
    "\n",
    "# we will import tqdm for progress bars\n",
    "from tqdm import tqdm # tqdm is a library for progress bars\n",
    "# strictly speaking we don't need to import tqdm but it makes our lives easier and it's nice to have progress bars\n",
    "\n",
    "# we will want to deal with web requests so requests provides a nice interface for that\n",
    "import requests # requests is a library for making web requests GET, POST, PUT, DELETE, etc.\n",
    "# it supports many features like authentication, cookies, sessions, etc.\n",
    "# more on it in your web scraping workshop\n",
    "\n",
    "import pandas as pd # pandas is a library for data analysis, commonly used in data science, machine learning, and artificial intelligence\n",
    "# notice how we renamed pandas to pd so we don't have to type out pandas every time we want to use it, this is very common\n",
    "# NOTE: Use common conventions when importing libraries. For example, pandas is commonly imported as pd, \n",
    "# numpy is commonly imported as np, matplotlib.pyplot is commonly imported as plt, etc.\n",
    "\n",
    "# for external libraries they will often provide a way to check the version of the library\n",
    "print(\"Pandas version:\", pd.__version__) # this is the version of pandas we are using\n",
    "# sometimes some functions might not work in older versions of the library so it's good to know what version you are using\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
