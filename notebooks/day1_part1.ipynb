{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Content Analysis - Setup \n",
    "\n",
    "![BSSDH](https://site-512948.mozfiles.com/files/512948/DHbaneris2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Text Content Analysis Workshop!\n",
    "\n",
    "### A few words about me\n",
    "\n",
    "Valdis Saulespurens \n",
    "\n",
    " * researcher at the National Library of Latvia, digital development department\n",
    " * 30+ years programming, 15 years with Python, 5 of those teaching Python\n",
    " * lecturer at Riga Technical University, Riga Business School\n",
    " * contact: valdis.saulespurens@lnb.lv - email, [Valdis on LinkedIn](https://www.linkedin.com/in/valdis-saulespurens), [ValRCS on Github](https://github.com/ValRCS)\n",
    "\n",
    "\n",
    "### A few words about the workshop\n",
    "\n",
    "* Two days - we meet on 25th and 26th of July of 2023 here at National Library of Latvia.\n",
    "#### Day 1 - 25th of July\n",
    "* Part 1 - 11:00 - 12:30 - Introduction to Text Content Analysis\n",
    "* Lunch Break 1 hour - 12:30 - 13:30\n",
    "* Part 2 - 13:30 - 15:00 - Cleaning, preprocessing and tokenization\n",
    "* Coffee Break 20 min \n",
    "* Part 3 - 15:20 - 16:50 - Creating embeddings \n",
    "\n",
    "#### Day 2 - 26th of July\n",
    "* Part 4 - 11:00 - 12:30 - Topic modeling\n",
    "* Lunch Break 1 hour - 12:30 - 13:30\n",
    "* Part 5 - 13:30 - 15:00 - Trend analysis, visualization and interpretation\n",
    "* Coffee Break 20 min\n",
    "* Part 6 - 15:20 - 16:50 - Your own work in class with the help of the instructor - required for the certificate/gaining credits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your computer setup\n",
    "\n",
    "This workshop will be using Python programming language and Jupyter Notebooks. Some minimal prerequisites are required to be able to run the notebooks in this repository.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "* You know a little bit about Python - refresher is provided in this repository\n",
    "* You have a Google account (gmail) and can use Google Colab\n",
    "\n",
    "For those new to Jupyter Notebooks - they are a way to combine text and code in a single document. You can run the code and see the results right in the notebook. You can also edit the code and run it again. This is a very convenient way to learn and experiment with Python. More on Jupyter Notebooks here : [Jupyter Notebooks](https://jupyter.org/)\n",
    "\n",
    "Jupyter Notebooks can be run locally on your computer or in the cloud. The primary/minimal option is using Google Colab - a cloud based Jupyter Notebook environment. You will need a Google account (gmail) to use Google Colab.\n",
    "\n",
    "I as an instructor will be using Visual Studio Code with Python extension and git support. This is a very powerful environment for Python development. You can install it on your computer and use it for this workshop. You will need to install Python and git on your computer - instructions were sent and are below. This is the preferred option for this workshop.\n",
    "\n",
    "### Test minimal prerequisites\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ValRCS/BSSDH_2023_workshop/blob/main/notebooks/test_python_setup.ipynb)\n",
    "\n",
    "You can run this same notebook in your own local environment if you have Python and git installed.\n",
    "\n",
    "\n",
    "### Practice Your Python Notebook skills (includes NumPy and Pandas library refresher)\n",
    "\n",
    "Here is a Python syntax refresher, optional if you have good working knowledge of Python.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ValRCS/BSSDH_2023_workshop/blob/main/notebooks/python_colab.ipynb)\n",
    "\n",
    "Again you can run this notebook in your own local environment if you have Python and git installed.\n",
    "\n",
    "### Local install instructions\n",
    "\n",
    "Detailed local install instructions can be found in INSTALL.md file in this repository. Link : [INSTALL.md](https://github.com/ValRCS/BSSDH_2023_workshop/blob/main/INSTALL.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Text Analysis\n",
    "\n",
    "### What is Text Analysis?\n",
    "\n",
    "Text analysis is the process of transforming unstructured text documents into structured data for further analysis. It is a form of data mining that is used to identify patterns and establish relationships between words in a text-based dataset. Text analysis is also known as text mining, text analytics, and data mining.\n",
    "\n",
    "### Why is Text Analysis Important in research and academia?\n",
    "\n",
    "Text analysis is important because it is a valuable method for extracting meaning from text-based data. It is used to quantify qualitative data, which is particularly helpful for research that involves collecting large amounts of unstructured data, such as customer feedback, open-ended survey responses, and social media comments.\n",
    "\n",
    "Text analysis is part of discourse analysis, which is the study of language use in texts and contexts. It is used to analyze the structure of written texts and is often used in the humanities and social sciences to analyze texts such as interview transcripts, news articles, and speeches.\n",
    "\n",
    "### Modeling Text Data after Preprocessing\n",
    "\n",
    "Text data is often modeled as numerical data after preprocessing. This is because most machine learning algorithms require numerical data as input. The most common way to model text data is to use a bag-of-words model, which represents each document as a vector of word counts. This is a simple and effective way to represent text data, but it does not capture the order of words in a document.\n",
    "We will be looking at some other ways to model text data in this workshop.\n",
    "\n",
    "## Pipeline - Plan of Attack\n",
    "\n",
    "### 1. Data Collection\n",
    "\n",
    "We will want to obtain data from some source. This could be a website, a database, or a file. We will be using a file for this workshop.\n",
    "However not every file is ready to be analyzed. We will need to clean the data and prepare it for analysis.\n",
    "\n",
    "Our first goal will be to read the data into a pandas dataframe. We will be using the `pandas` library for this. Pandas is a Python library that is used for data manipulation and analysis. It offers data structures and operations for manipulating numerical tables and time series. It also provides powerful data structures that are designed to make working with structured data fast, easy, and expressive.\n",
    "\n",
    "The process of importing data into a tabular format can be very easy or it can take quite some effort. We will work with medium sized datasets for this workshop. However, if you are working with truly large datasets, you may need to use a distributed computing framework such as Apache Spark to import the data. - this places huge demands on your hardware and is not recommended for beginners.\n",
    "Also Apache Spark is quite a bit slower when working with small datasets and few machines than regular Pandas based workflows.\n",
    "\n",
    "### 2. Data Preprocessing\n",
    "\n",
    "We will want to clean the data and prepare it for analysis. This includes removing punctuation, numbers, and other non-text characters. We will also want to remove stopwords, which are common words that do not add much meaning to a sentence, such as \"the\", \"and\", and \"a\". We will also want to remove words that appear too frequently or too infrequently in the dataset. This is known as removing words with high and low document frequency.\n",
    "\n",
    "### 3. Data Processing\n",
    "\n",
    "We will want to process the data in order to extract features from it. This includes tokenization, which is the process of splitting a text document into individual words. We will also want to stem and lemmatize the words in the dataset. Stemming is the process of reducing a word to its root form. Lemmatization is the process of reducing a word to its dictionary form. We will also want to remove words that are not nouns, verbs, adjectives, or adverbs. This is known as part-of-speech tagging.\n",
    "\n",
    "### 4. Creating Embeddings\n",
    "\n",
    "We will want to create embeddings from the data in order to represent the words in the dataset as vectors. This is known as word embedding. We will be using the `gensim` library for this. Gensim is a Python library for topic modeling, document indexing, and similarity retrieval with large corpora. It uses a fast implementation of the Word2Vec algorithm to learn vector representations of words.\n",
    "\n",
    "### 5. Modeling\n",
    "\n",
    "We will want to model the data in order to extract insights from it. This includes topic modeling, which is the process of discovering topics in a text-based dataset. We will might want to perform sentiment analysis, which is the process of determining whether a text document is positive, negative, or neutral. We will be using the `scikit-learn` library for this. Scikit-learn is a Python library for machine learning. It provides a range of supervised and unsupervised learning algorithms for classification, regression, and clustering.\n",
    "\n",
    "### 6. Visualization and Interpretation\n",
    "\n",
    "We will want to visualize the data in order to communicate our findings to others. This includes creating interactive visualizations using the Plotly library. Plotly is a Python library for creating interactive visualizations. It provides a range of tools for creating charts, maps, and graphs. We will also want to interpret the data in order to gain insights from it. This includes using the results of our analysis to make decisions about the data.\n",
    "\n",
    "For extra visualization of LDAModels we will be using pyLDAvis library - time permitting. It requires some extra setup and is not included in the Google Colab environment.\n",
    "\n",
    "### 7. Your own work in class  - required for the certificate/gaining credits\n",
    "\n",
    "In the last part of workshop you will be tasked with obtaining your own data and performing the same steps as in the workshop. You will be able to ask questions and get help from the instructor. This is required for the certificate/gaining credits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "### General Considerations and sources\n",
    "\n",
    "There are various sources of text data. Some of the most common sources are:\n",
    "\n",
    "* Web pages - see workshop on web scraping\n",
    "* Social media - generally need API access -- see issues with Twitter API\n",
    "* Books - see Project Gutenberg\n",
    "* News articles - see News API for organized access\n",
    "* Research papers - see ArXiv API\n",
    "* Wikipedia - see Wikipedia API\n",
    "* Blogs - see Blogger API or web scraping\n",
    "* Emails - see Enron Email Dataset as one example\n",
    "* Speeches - see American Presidency Project\n",
    "* Curated datasets - see Kaggle - note possible licensing issues\n",
    "* Dataset search - see [Google Dataset Search](https://datasetsearch.research.google.com/) - beware Killed By Google syndrome\n",
    "* US open data - see [data.gov](https://www.data.gov/)\n",
    "* European open data - see [European Data Portal](https://www.europeandataportal.eu/en)\n",
    "* Latvian open data - see [Latvian Open Data Portal](https://data.gov.lv/)\n",
    "* Your own data - see your own data you collected or have access to\n",
    "\n",
    "### Clarin - Reputable source of data\n",
    "\n",
    "Clarin is a European research infrastructure for language resources and technology. It is a networked federation of centres pooling their human and technical resources to create an infrastructure. The infrastructure consists of an interconnected network of repositories, service centres and knowledge centres, offering language resources (datasets) and natural language processing (NLP) tools and expertise. The infrastructure offers widespread access to language resources and advanced tools to support researchers in the humanities and social sciences, and beyond.\n",
    "\n",
    "[Clarin](https://www.clarin.eu/resource-families/historical-corpora)\n",
    "\n",
    "\n",
    "\n",
    "![Old Bailey](https://www.clarin.eu/sites/default/files/styles/large/public/media/showcases/Old_Bailey_Microcosm_edited_0.jpg?itok%253Dik2HYoSp)\n",
    "\n",
    "### Old Bailey Corpus\n",
    "\n",
    "The Old Bailey Corpus is a collection of aproximately 130 million words of text from the proceedings of the Old Bailey, a criminal court in London that operated from 1674 to 1913. The corpus is available for download from the Old Bailey Online website and other sources such as Clarin.\n",
    "\n",
    "Official Bailiff's website: [Old Bailey Online](https://www.oldbaileyonline.org/)\n",
    "\n",
    "Link at Clarin: [Old Bailey Corpus](https://www.clarin.eu/showcase/old-bailey-corpus-20-1720-1913)\n",
    "\n",
    "\n",
    "\n",
    "### Downloading the Data\n",
    "\n",
    "* Size: 134 Million Words\n",
    "* Annotation: detailed sociobiographical, pragmatic and textual annotation\n",
    "* Licence: CC-BY-NC-SA 4.0\n",
    "\n",
    "Note: Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License is a free license that allows you to share your work with others, as long as you credit the original author and indicate if changes were made. It also allows you to adapt the work and use it for non-commercial purposes, as long as you distribute it under the same license.\n",
    "\n",
    "More on Creative Commons Licenses: [Creative Commons](https://creativecommons.org/licenses/)\n",
    "\n",
    "Full size of the corpus is around 200MB, we will be using a selection of cases from each decade that is in the full corpus. This will be around 17MB of data uncompressed - around 3MB compressed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
