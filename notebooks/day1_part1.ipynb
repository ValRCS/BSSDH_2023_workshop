{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Content Analysis - Setup \n",
    "\n",
    "![BSSDH](https://site-512948.mozfiles.com/files/512948/DHbaneris2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Text Content Analysis Workshop!\n",
    "\n",
    "### A few words about me\n",
    "\n",
    "Valdis Saulespurens \n",
    "\n",
    " * researcher at the National Library of Latvia, digital development department\n",
    " * 30+ years programming, 15 years with Python, 5 of those teaching Python\n",
    " * lecturer at Riga Technical University, Riga Business School\n",
    " * contact: valdis.saulespurens@lnb.lv - email, [Valdis on LinkedIn](https://www.linkedin.com/in/valdis-saulespurens), [ValRCS on Github](https://github.com/ValRCS)\n",
    "\n",
    "\n",
    "### A few words about the workshop\n",
    "\n",
    "* Two days - we meet on 25th and 26th of July of 2023 here at National Library of Latvia.\n",
    "#### Day 1 - 25th of July\n",
    "* Part 1 - 11:00 - 12:30 - Introduction to Text Content Analysis\n",
    "* Lunch Break 1 hour - 12:30 - 13:30\n",
    "* Part 2 - 13:30 - 15:00 - Cleaning, preprocessing and tokenization\n",
    "* Coffee Break 20 min \n",
    "* Part 3 - 15:20 - 16:50 - Creating embeddings \n",
    "\n",
    "#### Day 2 - 26th of July\n",
    "* Part 4 - 11:00 - 12:30 - Topic modeling\n",
    "* Lunch Break 1 hour - 12:30 - 13:30\n",
    "* Part 5 - 13:30 - 15:00 - Trend analysis, visualization and interpretation\n",
    "* Coffee Break 20 min\n",
    "* Part 6 - 15:20 - 16:50 - Your own work in class with the help of the instructor - required for the certificate/gaining credits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your computer setup\n",
    "\n",
    "This workshop will be using Python programming language and Jupyter Notebooks. Some minimal prerequisites are required to be able to run the notebooks in this repository.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "* You know a little bit about Python - refresher is provided in this repository\n",
    "* You have a Google account (gmail) and can use Google Colab\n",
    "\n",
    "For those new to Jupyter Notebooks - they are a way to combine text and code in a single document. You can run the code and see the results right in the notebook. You can also edit the code and run it again. This is a very convenient way to learn and experiment with Python. More on Jupyter Notebooks here : [Jupyter Notebooks](https://jupyter.org/)\n",
    "\n",
    "Jupyter Notebooks can be run locally on your computer or in the cloud. The primary/minimal option is using Google Colab - a cloud based Jupyter Notebook environment. You will need a Google account (gmail) to use Google Colab.\n",
    "\n",
    "I as an instructor will be using Visual Studio Code with Python extension and git support. This is a very powerful environment for Python development. You can install it on your computer and use it for this workshop. You will need to install Python and git on your computer - instructions were sent and are below. This is the preferred option for this workshop.\n",
    "\n",
    "### Test minimal prerequisites\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ValRCS/BSSDH_2023_workshop/blob/main/notebooks/test_python_setup.ipynb)\n",
    "\n",
    "You can run this same notebook in your own local environment if you have Python and git installed.\n",
    "\n",
    "\n",
    "### Practice Your Python Notebook skills (includes NumPy and Pandas library refresher)\n",
    "\n",
    "Here is a Python syntax refresher, optional if you have good working knowledge of Python.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ValRCS/BSSDH_2023_workshop/blob/main/notebooks/python_colab.ipynb)\n",
    "\n",
    "Again you can run this notebook in your own local environment if you have Python and git installed.\n",
    "\n",
    "### Local install instructions\n",
    "\n",
    "Detailed local install instructions can be found in INSTALL.md file in this repository. Link : [INSTALL.md](https://github.com/ValRCS/BSSDH_2023_workshop/blob/main/INSTALL.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Text Analysis\n",
    "\n",
    "### What is Text Analysis?\n",
    "\n",
    "Text analysis is the process of transforming unstructured text documents into structured data for further analysis. It is a form of data mining that is used to identify patterns and establish relationships between words in a text-based dataset. Text analysis is also known as text mining, text analytics, and data mining.\n",
    "\n",
    "### Why is Text Analysis Important in research and academia?\n",
    "\n",
    "Text analysis is important because it is a valuable method for extracting meaning from text-based data. It is used to quantify qualitative data, which is particularly helpful for research that involves collecting large amounts of unstructured data, such as customer feedback, open-ended survey responses, and social media comments.\n",
    "\n",
    "Text analysis is part of discourse analysis, which is the study of language use in texts and contexts. It is used to analyze the structure of written texts and is often used in the humanities and social sciences to analyze texts such as interview transcripts, news articles, and speeches.\n",
    "\n",
    "### Modeling Text Data after Preprocessing\n",
    "\n",
    "Text data is often modeled as numerical data after preprocessing. This is because most machine learning algorithms require numerical data as input. The most common way to model text data is to use a bag-of-words model, which represents each document as a vector of word counts. This is a simple and effective way to represent text data, but it does not capture the order of words in a document.\n",
    "We will be looking at some other ways to model text data in this workshop.\n",
    "\n",
    "## Pipeline - Plan of Attack\n",
    "\n",
    "### 1. Data Collection\n",
    "\n",
    "We will want to obtain data from some source. This could be a website, a database, or a file. We will be using a file for this workshop.\n",
    "However not every file is ready to be analyzed. We will need to clean the data and prepare it for analysis.\n",
    "\n",
    "Our first goal will be to read the data into a pandas dataframe. We will be using the `pandas` library for this. Pandas is a Python library that is used for data manipulation and analysis. It offers data structures and operations for manipulating numerical tables and time series. It also provides powerful data structures that are designed to make working with structured data fast, easy, and expressive.\n",
    "\n",
    "The process of importing data into a tabular format can be very easy or it can take quite some effort. We will work with medium sized datasets for this workshop. However, if you are working with truly large datasets, you may need to use a distributed computing framework such as Apache Spark to import the data. - this places huge demands on your hardware and is not recommended for beginners.\n",
    "Also Apache Spark is quite a bit slower when working with small datasets and few machines than regular Pandas based workflows.\n",
    "\n",
    "### 2. Data Preprocessing\n",
    "\n",
    "We will want to clean the data and prepare it for analysis. This includes removing punctuation, numbers, and other non-text characters. We will also want to remove stopwords, which are common words that do not add much meaning to a sentence, such as \"the\", \"and\", and \"a\". We will also want to remove words that appear too frequently or too infrequently in the dataset. This is known as removing words with high and low document frequency.\n",
    "\n",
    "### 3. Data Processing\n",
    "\n",
    "We will want to process the data in order to extract features from it. This includes tokenization, which is the process of splitting a text document into individual words. We will also want to stem and lemmatize the words in the dataset. Stemming is the process of reducing a word to its root form. Lemmatization is the process of reducing a word to its dictionary form. We will also want to remove words that are not nouns, verbs, adjectives, or adverbs. This is known as part-of-speech tagging.\n",
    "\n",
    "### 4. Creating Embeddings\n",
    "\n",
    "We will want to create embeddings from the data in order to represent the words in the dataset as vectors. This is known as word embedding. We will be using the `gensim` library for this. Gensim is a Python library for topic modeling, document indexing, and similarity retrieval with large corpora. It uses a fast implementation of the Word2Vec algorithm to learn vector representations of words.\n",
    "\n",
    "### 5. Modeling\n",
    "\n",
    "We will want to model the data in order to extract insights from it. This includes topic modeling, which is the process of discovering topics in a text-based dataset. We will might want to perform sentiment analysis, which is the process of determining whether a text document is positive, negative, or neutral. We will be using the `scikit-learn` library for this. Scikit-learn is a Python library for machine learning. It provides a range of supervised and unsupervised learning algorithms for classification, regression, and clustering.\n",
    "\n",
    "### 6. Visualization and Interpretation\n",
    "\n",
    "We will want to visualize the data in order to communicate our findings to others. This includes creating interactive visualizations using the Plotly library. Plotly is a Python library for creating interactive visualizations. It provides a range of tools for creating charts, maps, and graphs. We will also want to interpret the data in order to gain insights from it. This includes using the results of our analysis to make decisions about the data.\n",
    "\n",
    "For extra visualization of LDAModels we will be using pyLDAvis library - time permitting. It requires some extra setup and is not included in the Google Colab environment.\n",
    "\n",
    "### 7. Your own work in class  - required for the certificate/gaining credits\n",
    "\n",
    "In the last part of workshop you will be tasked with obtaining your own data and performing the same steps as in the workshop. You will be able to ask questions and get help from the instructor. This is required for the certificate/gaining credits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "### General Considerations and sources\n",
    "\n",
    "There are various sources of text data. Some of the most common sources are:\n",
    "\n",
    "* Web pages - see workshop on web scraping\n",
    "* Social media - generally need API access -- see issues with Twitter API\n",
    "* Books - see Project Gutenberg\n",
    "* News articles - see News API for organized access\n",
    "* Research papers - see ArXiv API\n",
    "* Wikipedia - see Wikipedia API\n",
    "* Blogs - see Blogger API or web scraping\n",
    "* Emails - see Enron Email Dataset as one example\n",
    "* Speeches - see American Presidency Project\n",
    "* Curated datasets - see Kaggle - note possible licensing issues\n",
    "* Dataset search - see [Google Dataset Search](https://datasetsearch.research.google.com/) - beware Killed By Google syndrome\n",
    "* US open data - see [data.gov](https://www.data.gov/)\n",
    "* European open data - see [European Data Portal](https://www.europeandataportal.eu/en)\n",
    "* Latvian open data - see [Latvian Open Data Portal](https://data.gov.lv/)\n",
    "* Your own data - see your own data you collected or have access to\n",
    "\n",
    "### Clarin - Reputable source of data\n",
    "\n",
    "Clarin is a European research infrastructure for language resources and technology. It is a networked federation of centres pooling their human and technical resources to create an infrastructure. The infrastructure consists of an interconnected network of repositories, service centres and knowledge centres, offering language resources (datasets) and natural language processing (NLP) tools and expertise. The infrastructure offers widespread access to language resources and advanced tools to support researchers in the humanities and social sciences, and beyond.\n",
    "\n",
    "[Clarin](https://www.clarin.eu/resource-families/historical-corpora)\n",
    "\n",
    "\n",
    "\n",
    "![Old Bailey](https://www.clarin.eu/sites/default/files/styles/large/public/media/showcases/Old_Bailey_Microcosm_edited_0.jpg?itok%253Dik2HYoSp)\n",
    "\n",
    "### Old Bailey Corpus\n",
    "\n",
    "The Old Bailey Corpus is a collection of aproximately 130 million words of text from the proceedings of the Old Bailey, a criminal court in London that operated from 1674 to 1913. The corpus is available for download from the Old Bailey Online website and other sources such as Clarin.\n",
    "\n",
    "Official Bailiff's website: [Old Bailey Online](https://www.oldbaileyonline.org/)\n",
    "\n",
    "Link at Clarin: [Old Bailey Corpus](https://www.clarin.eu/showcase/old-bailey-corpus-20-1720-1913)\n",
    "\n",
    "\n",
    "\n",
    "### Downloading the Data\n",
    "\n",
    "* Size: 134 Million Words\n",
    "* Annotation: detailed sociobiographical, pragmatic and textual annotation\n",
    "* Licence: CC-BY-NC-SA 4.0\n",
    "\n",
    "Note: Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License is a free license that allows you to share your work with others, as long as you credit the original author and indicate if changes were made. It also allows you to adapt the work and use it for non-commercial purposes, as long as you distribute it under the same license.\n",
    "\n",
    "More on Creative Commons Licenses: [Creative Commons](https://creativecommons.org/licenses/)\n",
    "\n",
    "Full size of the corpus is around 200MB, we will be using a selection of cases from each decade that is in the full corpus. This will be around 17MB of data uncompressed - around 3MB compressed.\n",
    "\n",
    "\n",
    "### Some considerations on downloading data\n",
    "\n",
    "* Data is available in various formats - we will be using XML format - this is a text based format that is supposed to be human readable, and computer parsable\n",
    "* Data is available in various sizes - we will be using a subset of the data - around 17MB uncompressed\n",
    "* We will be downloading the data from github repository - this is a very convenient way to share data and code as long as the data is not too large - github has a 100MB limit on file size without extra setup\n",
    "* if you are using local setup your data will already be in data folder - you could skip the download step but try to follow the rest of the steps\n",
    "* Google Colab does not provide automatic access to your local files - you will need to upload the data to Google Colab environment - this is a bit cumbersome but doable\n",
    "* We will be using a python library called `requests` to download the data - this is a very convenient way to download data from the internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Good Practices in creating notebooks\n",
    "\n",
    "* Use markdown cells to explain what you are doing and why\n",
    "* See Markdown cheatsheet at Github : [Markdown Cheatsheet](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)\n",
    "* Markdown headings can serve as table of contents for your notebook - use only one level 1 heading\n",
    "* Use comments in code cells to explain what you are doing and why\n",
    "* Use descriptive variable names - avoid single letter variable names\n",
    "* Use descriptive function names - avoid single letter function names\n",
    "* Use descriptive file names - avoid single letter file names\n",
    "* Use descriptive folder names - avoid single letter folder names - you get the idea by now :)\n",
    "* Use version control - git is the most popular version control system - we will be using it in this workshop\n",
    "* Note: git is not ideal for notebooks (JSON format) - it is better suited for pure code files - but it is better than nothing\n",
    "* Your notebook should be able to run all the way through without errors - this is not always possible but should be the goal\n",
    "* In other words your notebook should be able to be exported and run as standalone python file in ideal case - not always possible but should be the goal\n",
    "\n",
    "Finally, remember creating notebooks is a fusion of logical and creative process and you might not always know where you will end up. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]\n",
      "Pandas version: 2.0.3\n"
     ]
    }
   ],
   "source": [
    "## Finally we can get started with some real coding!\n",
    "\n",
    "## First we will want to import all the libraries need for this particular notebook\n",
    "\n",
    "## Note: When developing your notebook it might be okay to import libraries as you need them, but when you are done with your notebook you should move all your imports to the top of the notebook.\n",
    "\n",
    "## First we want to import standard libraries that come with python. These are libraries that are installed with python and you don't have to install them yourself.\n",
    "\n",
    "# we might need sys to get the version of python we are using and possibly other things\n",
    "import sys\n",
    "# print python version\n",
    "print(\"Python version:\", sys.version) # not really needed but good to know in case something goes wrong\n",
    "\n",
    "# we will want Path module from pathlib to work with paths\n",
    "from pathlib import Path # notice how we only import Path from pathlib and not the whole library\n",
    "\n",
    "# we will be forking with zipfiles so we will need zipfile\n",
    "import zipfile\n",
    "\n",
    "# we will be working with xml files so we will need xml\n",
    "import xml.etree.ElementTree as ET # so we imported xml but we only imported the ElementTree module from xml\n",
    "# we also renamed ElementTree to ET so we don't have to type out ElementTree every time we want to use it\n",
    "\n",
    "# we might need some regular expression magic\n",
    "import re\n",
    "\n",
    "# we might want to deal with some json data\n",
    "import json # also standard library\n",
    "\n",
    "# we might want to deal with some dates\n",
    "from datetime import datetime # also standard library\n",
    "# note how we imported datetime from datetime, this is because datetime is a module and a class in the datetime library\n",
    "# a bit of unfortunate naming but we can deal with it\n",
    "\n",
    "# we might need some random selections\n",
    "import random # also standard library\n",
    "\n",
    "## External Libraries\n",
    "## These are libraries that are not installed with python and you have to install them yourself. You can install them with pip or conda. We will use pip for this class.\n",
    "## Note: You can also import libraries with an alias. This is useful when you want to use a library but don't want to type out the whole name every time you use it. For example, we can import pandas as pd. This will allow us to use pandas but we only have to type pd when we want to use it.\n",
    "\n",
    "# we will import tqdm for progress bars\n",
    "from tqdm import tqdm # tqdm is a library for progress bars\n",
    "# strictly speaking we don't need to import tqdm but it makes our lives easier and it's nice to have progress bars\n",
    "\n",
    "# we will want to deal with web requests so requests provides a nice interface for that\n",
    "import requests # requests is a library for making web requests GET, POST, PUT, DELETE, etc.\n",
    "# it supports many features like authentication, cookies, sessions, etc.\n",
    "# more on it in your web scraping workshop\n",
    "\n",
    "import pandas as pd # pandas is a library for data analysis, commonly used in data science, machine learning, and artificial intelligence\n",
    "# notice how we renamed pandas to pd so we don't have to type out pandas every time we want to use it, this is very common\n",
    "# NOTE: Use common conventions when importing libraries. For example, pandas is commonly imported as pd, \n",
    "# numpy is commonly imported as np, matplotlib.pyplot is commonly imported as plt, etc.\n",
    "\n",
    "# for external libraries they will often provide a way to check the version of the library\n",
    "print(\"Pandas version:\", pd.__version__) # this is the version of pandas we are using\n",
    "# sometimes some functions might not work in older versions of the library so it's good to know what version you are using\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://github.com/ValRCS/BSSDH_2023_workshop/raw/main/data/old_bailey_sample_1720_1913.zip\n"
     ]
    }
   ],
   "source": [
    "# our first step is to find out url where our data is located\n",
    "# we can find this out by looking at the source code of the webpage\n",
    "# in this case if you check Github repository of this project you will find that Github will \n",
    "# provide an option to download data raw\n",
    "# you could do this manually by left clickin raw button in \n",
    "# https://github.com/ValRCS/BSSDH_2023_workshop/blob/main/data/old_bailey_sample_1720_1913.zip\n",
    "# alternatively you can right click raw button and click copy link address\n",
    "url = \"https://github.com/ValRCS/BSSDH_2023_workshop/raw/main/data/old_bailey_sample_1720_1913.zip\"\n",
    "# note raw part of the url, this is important because it will allow us to download the data directly\n",
    "# in general there would be some way to figure out the url but it's not always easy\n",
    "# let's print our url to make sure it's correct\n",
    "print(\"URL:\", url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: d:\\Github\\BSSDH_2023_workshop\\notebooks\n",
      "Temp folder exists: True\n"
     ]
    }
   ],
   "source": [
    "# now we want to download the data and copy into temp folder\n",
    "# first we will want to creata a temp folder if it does not exist\n",
    "# to keep notebooks on the same path we will use Path.cwd() to get the current working directory\n",
    "# print current working directory\n",
    "print(\"Current working directory:\", Path.cwd())\n",
    "# then we will use Path.mkdir() to create a folder\n",
    "# we will use exist_ok=True to make sure we don't get an error if the folder already exists\n",
    "Path(Path.cwd() / \"temp\").mkdir(exist_ok=True)\n",
    "# check that we created the folder\n",
    "print(\"Temp folder exists:\", Path(Path.cwd() / \"temp\").exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save file as: old_bailey_sample_1720_1913.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "old_bailey_sample_1720_1913.zip: 100%|██████████| 2.90M/2.90M [00:10<00:00, 278kB/s] \n"
     ]
    }
   ],
   "source": [
    "# finally we are ready to download the data\n",
    "# we will use requests to download the data\n",
    "# we will use requests.get() to make a GET request to the url\n",
    "# we will use stream=True to stream the data instead of downloading it all at once\n",
    "# we will use allow_redirects=True to allow redirects\n",
    "# we will use timeout=10 to timeout after 10 seconds\n",
    "\n",
    "# we will use with statement to open a file\n",
    "# we will use open() to open a file\n",
    "# we will use \"wb\" to open a file in binary mode\n",
    "# we will use Path() to create a path to our file\n",
    "# we will use Path.cwd() to get the current working directory   \n",
    "# we will use / to join paths\n",
    "# we will use \"temp\" to create a path to our file\n",
    "\n",
    "# we will use tqdm() to create a progress bar\n",
    "\n",
    "# we will use .iter_content() to iterate over the content of the response\n",
    "# we will use chunk_size=1024 to iterate over the content in chunks of 1024 bytes\n",
    "\n",
    "# we will use .write() to write the content to the file\n",
    "\n",
    "# we will use .close() to close the file when we are done\n",
    "\n",
    "# we will use .raise_for_status() to raise an exception if the status code is not 200\n",
    "\n",
    "# we will get file name from our url it is the last part of the url after the last /\n",
    "# we will use .split() to split the url by /\n",
    "file_name = url.split(\"/\")[-1] # so we split the url by / and get the last part of the url\n",
    "print(\"Will save file as:\", file_name)\n",
    "\n",
    "# now let's download the data - we are chunking data to support large files\n",
    "\n",
    "with open(Path(Path.cwd() / \"temp\" / file_name), \"wb\") as file: # note wb - write binary\n",
    "    # to speed up the download we will use stream=True\n",
    "    # also we are using chunk_size=1024 to download the data in chunks of 1024 bytes\n",
    "    # you can adjust chunk_size to your liking\n",
    "    with requests.get(url, stream=True, allow_redirects=True, timeout=10) as response:\n",
    "        with tqdm(total=int(response.headers.get(\"content-length\", 0)), unit=\"B\", unit_scale=True, desc=file_name) as progress:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                file.write(chunk)\n",
    "                progress.update(len(chunk))\n",
    "        # file.close() # not required because of with statement\n",
    "        response.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 2900720\n"
     ]
    }
   ],
   "source": [
    "# now those on local machine can check that the data was downloaded to temp folder and is identical to same file in data folder\n",
    "# you can also check that the file size is the same\n",
    "# print file size\n",
    "print(\"File size:\", Path(Path.cwd() / \"temp\" / file_name).stat().st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original file size: 2900720\n"
     ]
    }
   ],
   "source": [
    "# print file size of original file - will not work in Colab!\n",
    "# print(\"Original file size:\", Path(Path.cwd().parent / \"data\" / file_name).stat().st_size)\n",
    "## same file size is generally a good sign that the files are identical - but not always"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD5: 66c094035ceee02b306d110adf7cf658\n"
     ]
    }
   ],
   "source": [
    "# for more hardcore people you can use checksums to check that the files are identical\n",
    "# you can use md5, sha1, sha256, etc.\n",
    "# you can use hashlib to calculate checksums\n",
    "import hashlib\n",
    "# we will use md5\n",
    "md5 = hashlib.md5()\n",
    "#\n",
    "# we will use .read_bytes() to read the file as bytes\n",
    "# we will use .update() to update the md5 hash\n",
    "md5.update(Path(Path.cwd() / \"temp\" / file_name).read_bytes())\n",
    "# we will use .hexdigest() to get the md5 hash as a string\n",
    "print(\"MD5:\", md5.hexdigest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# advantage of above approach is that you can check that the file is identical to the original file without having two copies of the file\n",
    "# you can also check that the file is identical to the original file without having to download the original file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are going to unzip d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\old_bailey_sample_1720_1913.zip\n",
      "We are going to unzip to d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\n"
     ]
    }
   ],
   "source": [
    "# So now we have downloaded the data and we can start working with it\n",
    "# first we will want to unzip the data\n",
    "# the simplest would be to use zipfile and just extract all of the files in original zip file keeping folder structure\n",
    "\n",
    "# let's do that for now\n",
    "\n",
    "# first we will want to create a path to our zip file\n",
    "zip_path = Path(Path.cwd() / \"temp\" / file_name)\n",
    "print(\"We are going to unzip\", zip_path) # the absolute path will be differnt on each machine!\n",
    "\n",
    "# unzip folder will be same temp folder\n",
    "unzip_folder = Path(Path.cwd() / \"temp\")\n",
    "print(\"We are going to unzip to\", unzip_folder) # the absolute path will be differnt on each machine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's unzip our zip file\n",
    "# we will use zipfile.ZipFile() to create a ZipFile object\n",
    "\n",
    "# we will use .extractall() to extract all files from the zip file to the unzip folder\n",
    "\n",
    "with zipfile.ZipFile(zip_path) as zip_file:\n",
    "    zip_file.extractall(unzip_folder) # extract all files to unzip folder keeping folder structure\n",
    "\n",
    "# there are other recipes for working with truly large zip files where you might not want to unzip the whole file at once\n",
    "# but for now we will keep it simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\LICENSE.txt\n",
      "Directory: d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\n",
      "File: d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\old_bailey_sample_1720_1913.csv\n",
      "File: d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\old_bailey_sample_1720_1913.parquet\n",
      "File: d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\old_bailey_sample_1720_1913.zip\n",
      "File: d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\old_bailey_sample_1720_1913_cleaned.csv\n",
      "File: d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\old_bailey_sample_1720_1913_cleaned.parquet\n",
      "File: d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\README.html\n"
     ]
    }
   ],
   "source": [
    "# we can see that there was some file structure in the zip file\n",
    "# we could use our file explorer or we could list the directory in temp\n",
    "# we will use Path.iterdir() to iterate over the files in the temp folder and print them\n",
    "for file in Path(unzip_folder).iterdir():\n",
    "    # if file is a directory we will print it as a directory\n",
    "    if file.is_dir():\n",
    "        print(\"Directory:\", file)\n",
    "    # if file is a file we will print it as a file\n",
    "    elif file.is_file():\n",
    "        print(\"File:\", file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 28\n",
      "First 10 files:\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\LICENSE.txt\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\old_bailey_sample_1720_1913.csv\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\old_bailey_sample_1720_1913.parquet\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\old_bailey_sample_1720_1913.zip\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\old_bailey_sample_1720_1913_cleaned.csv\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\old_bailey_sample_1720_1913_cleaned.parquet\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\README.html\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17200427.xml\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17310428.xml\n"
     ]
    }
   ],
   "source": [
    "# we can use Path.rglob() to recursively iterate over all files in the temp folder\n",
    "# we will use ** to recursively iterate over all files in the temp folder\n",
    "# we will use * to iterate over all files in the temp folder\n",
    "\n",
    "all_files = [file for file in Path(unzip_folder).rglob(\"*\")]\n",
    "print(\"Number of files:\", len(all_files))\n",
    "# print first 10 files\n",
    "print(\"First 10 files:\", *all_files[:10], sep=\"\\n\") # i unrolled the first 10 files with * and separated them with new line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of xml files: 20\n",
      "First 10 xml files:\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17200427.xml\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17310428.xml\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17420428.xml\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17540116.xml\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17620224.xml\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17731020.xml\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17841210.xml\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17961026.xml\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-18020217.xml\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-18140112.xml\n"
     ]
    }
   ],
   "source": [
    "# now we can see that we have a lot of files\n",
    "# how about only getting xml files because those are in fact what we want\n",
    "# let's use list comprehension to get only xml files\n",
    "xml_files = [file for file in Path(unzip_folder).rglob(\"*.xml\")]\n",
    "print(\"Number of xml files:\", len(xml_files))\n",
    "# print first 10 xml files\n",
    "print(\"First 10 xml files:\", *xml_files[:10], sep=\"\\n\") # i unrolled the first 10 files with * and separated them with new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of xml files: 20\n",
      "First 5 xml files:\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17200427.xml\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17310428.xml\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17420428.xml\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17540116.xml\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17620224.xml\n",
      "First 5 xml files: [WindowsPath('d:/Github/BSSDH_2023_workshop/notebooks/temp/obc_texts/OBC2-17200427.xml'), WindowsPath('d:/Github/BSSDH_2023_workshop/notebooks/temp/obc_texts/OBC2-17310428.xml'), WindowsPath('d:/Github/BSSDH_2023_workshop/notebooks/temp/obc_texts/OBC2-17420428.xml'), WindowsPath('d:/Github/BSSDH_2023_workshop/notebooks/temp/obc_texts/OBC2-17540116.xml'), WindowsPath('d:/Github/BSSDH_2023_workshop/notebooks/temp/obc_texts/OBC2-17620224.xml')]\n"
     ]
    }
   ],
   "source": [
    "# let's get a list version of the above code using regular loop\n",
    "# so list comprehension is just a fancy way of writing a for loop\n",
    "\n",
    "xml_files_2 = [] # start with empty list\n",
    "for file in Path(unzip_folder).rglob(\"*.xml\"): # iterate over all files in temp folder recursively\n",
    "    # rglob returns generators, generators are great \n",
    "    # for looping over large datasets because they don't load everything into memory\n",
    "    # i could add some extra logic maybe for size or date modified\n",
    "    xml_files_2.append(file) # append file to list\n",
    "# print how many we got\n",
    "print(\"Number of xml files:\", len(xml_files_2))\n",
    "# first 5 xml files\n",
    "print(\"First 5 xml files:\", *xml_files_2[:5], sep=\"\\n\") # i unrolled the first 5 files with * and \n",
    "# separated them with new line\n",
    "# I could print without unrolling but it would be harder to read\n",
    "print(\"First 5 xml files:\", xml_files_2[:5]) # i unrolled the first 5 files with * and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 xml files sorted by name:\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17200427.xml\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17310428.xml\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17420428.xml\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17540116.xml\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17620224.xml\n"
     ]
    }
   ],
   "source": [
    "## we could provide various ordering of files, let's sort by file name since that is the simplest\n",
    "# we are using sort method for list - IN PLACE means modifies the list\n",
    "xml_files.sort(key=lambda x: x.name)\n",
    "print(\"First 5 xml files sorted by name:\", *xml_files[:5], sep=\"\\n\") # i unrolled the first 10 files with * and separated them with new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-18211205.xml 1754682\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-18020217.xml 1651660\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-18351214.xml 1391665\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-18140112.xml 1136176\n",
      "d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-19020909.xml 1121205\n"
     ]
    }
   ],
   "source": [
    "# let's sort by file size in a new list\n",
    "# we OUT OF PLACE sorted - does not change original list\n",
    "xml_files_sorted_by_size = sorted(xml_files, key=lambda x: x.stat().st_size)\n",
    "# let's print 5 largest files and their respective sizes\n",
    "for file in xml_files_sorted_by_size[-5:][::-1]: # so i took last 5 files and reversed them\n",
    "    # note: for iterating/looping through larger collections\n",
    "    # reversed would have been better because it does not load everything into memory\n",
    "    print(file, file.stat().st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will open and parse the following file: d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\obc_texts\\OBC2-17200427.xml\n"
     ]
    }
   ],
   "source": [
    "first_file = xml_files[0]\n",
    "print(\"I will open and parse the following file:\", first_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root tag: TEI.2\n"
     ]
    }
   ],
   "source": [
    "# looks like the first file by name is indeed the oldest file from 1720\n",
    "# let's load it in and see what we have\n",
    "# we will use xml.etree.ElementTree.parse() to parse the xml file\n",
    "# we will use .getroot() to get the root element of the xml file\n",
    "# we will use .tag to get the tag of the root element\n",
    "# we will use .attrib to get the attributes of the root element\n",
    "\n",
    "# let's get started\n",
    "\n",
    "tree = ET.parse(xml_files[0]) # parse the xml file\n",
    "# the big assumption is that it is a valid xml file\n",
    "root = tree.getroot() # get the root element\n",
    "print(\"Root tag:\", root.tag) # print the tag of the root element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEI - Text Encoding Initiative\n",
    "\n",
    "What is TEI?\n",
    "\n",
    "TEI - Text Encoding Initiative is a consortium which collectively develops and maintains a standard for the representation of texts in digital form. Its chief deliverable is a set of Guidelines which specify encoding methods for machine-readable texts, chiefly in the humanities, social sciences and linguistics. Since 1994, the TEI Guidelines have been widely used by libraries, museums, publishers, and individual scholars to present texts for online research, teaching, and preservation. In addition to the Guidelines themselves, the Consortium provides a variety of resources and training events for learning TEI, information on projects using the TEI, a bibliography of TEI-related publications, and software developed for or adapted to the TEI.\n",
    "\n",
    "### TEI and XML\n",
    "\n",
    "![TEI](https://tei-c.org/release/doc/tei-p5-doc/en/html/Images/banner.jpg)\n",
    "\n",
    "[TEI](https://tei-c.org/) is a standard for encoding text in XML. \n",
    "\n",
    "XML is a markup language that is used to describe data. XML is extremely flexible which is both a strength and a weakness. It is a strength because it allows you to create your own tags and attributes. It is a weakness because it can be difficult to understand and maintain.\n",
    "\n",
    "### XML Tutorial\n",
    "\n",
    "XML resembles HTMl in some ways as they share a common ancestor. XML is a markup language that is used to describe data. \n",
    "\n",
    "A basic tutorial on XML can be found here: [XML Tutorial](https://www.w3schools.com/xml/) - W3Schools is not officially affiliated with W3C. It used to be less reputable but has improved over the years.\n",
    "\n",
    "For additional complexity you can use namespaces to create your own tags and attributes. This is a way to avoid name collisions with other XML tags and attributes. This can add additional complexity to your code.\n",
    "\n",
    "TEI provides its own standart XML tags:\n",
    "See TEI documentation: [TEI](https://tei-c.org/release/doc/tei-p5-doc/en/html/index.html)\n",
    "\n",
    "A good starting point for sample document might here: [Default Document](https://tei-c.org/release/doc/tei-p5-doc/en/html/DS.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use any standart text editor to open the xml file and see what it looks like\n",
    "# I will be using this very same Visual Studio Code\n",
    "\n",
    "# upon inspection looks like I will be interested in div1 tags of type trialAccount\n",
    "# our next task will be to extract all the trialAccount div1 tags and save plain text of those div1 tags\n",
    "# we will also want to have some metadata about the trialAccount div1 tags such as year, trial number, and trial date if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trialAccount div1 tags: 75\n"
     ]
    }
   ],
   "source": [
    "# let's get a list of all div1 tags of type trialAccount\n",
    "# we will use .findall() to find all div1 tags\n",
    "# we will use .iter() to iterate over all div1 tags\n",
    "# we will use .attrib to get the attributes of the div1 tag\n",
    "# we will use .get() to get the value of the attribute\n",
    "# we will use .text to get the text of the div1 tag\n",
    "\n",
    "# we will use list comprehension to get all div1 tags of type trialAccount\n",
    "trial_accounts = [div1 for div1 in root.findall(\".//div1[@type='trialAccount']\")] \n",
    "# note we are using .//div1[@type='trialAccount'] to find all div1 tags of type trialAccount\n",
    "# this is using xpath syntax\n",
    "# more about xpath here: https://www.w3schools.com/xml/xpath_intro.asp\n",
    "print(\"Number of trialAccount div1 tags:\", len(trial_accounts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trialAccount div1 tags: 75\n"
     ]
    }
   ],
   "source": [
    "# if xpath is not your thing you can use .iter() to iterate over all div1 tags and check if they have type attribute and if it's value is trialAccount\n",
    "# we will use .iter() to iterate over all div1 tags\n",
    "# we will use .attrib to get the attributes of the div1 tag\n",
    "# we will use .get() to get the value of the attribute\n",
    "# we will use .text to get the text of the div1 tag\n",
    "\n",
    "# we will use a simple loop\n",
    "trial_accounts_too = []\n",
    "for div1 in root.iter(\"div1\"): # iterate over all div1 tags\n",
    "    if \"type\" in div1.attrib: # check if div1 tag has type attribute\n",
    "        if div1.attrib[\"type\"] == \"trialAccount\": # check if div1 tag has type attribute with value trialAccount\n",
    "            trial_accounts_too.append(div1) # if it does append it to trial_accounts_too\n",
    "print(\"Number of trialAccount div1 tags:\", len(trial_accounts_too))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trialAccount div1 tags: 75\n"
     ]
    }
   ],
   "source": [
    "## we could use get method for attributes\n",
    "trial_accounts_three = []\n",
    "for div1 in root.iter(\"div1\"):\n",
    "    if div1.attrib.get(\"type\") == \"trialAccount\": # None is actually the default for get method\n",
    "        trial_accounts_three.append(div1)\n",
    "print(\"Number of trialAccount div1 tags:\", len(trial_accounts_three))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First trial text: \n"
     ]
    }
   ],
   "source": [
    "# now let's see if we can extract all text from first trial\n",
    "# we already have trial_accounts list so we can use that\n",
    "# we will use .text to get the text of the div1 tag\n",
    "# we will use .strip() to strip the text of the div1 tag of leading and trailing whitespace\n",
    "# we will use .replace(\"\\n\", \" \") to replace new line characters with space\n",
    "# we will use .replace(\"\\t\", \" \") to replace tab characters with space\n",
    "# we will use .replace(\"\\r\", \" \") to replace carriage return characters with space\n",
    "\n",
    "first_trial_text = trial_accounts[0].text.strip().replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"\\r\", \" \")\n",
    "print(\"First trial text:\", first_trial_text)\n",
    "# we get nothing because text is in child tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First trial text: \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      " \n",
      "  \n",
      " \n",
      "  \n",
      "  \n",
      "   \n",
      " ,of  St. Leonard Eastcheap  \n",
      "  \n",
      "  \n",
      "  , was indicted for  \n",
      "  \n",
      "  \n",
      " feloniously stealing 70 Pound weight of Tobacco, value 4 l. 10 s. the Goods of \n",
      "  \n",
      " Job \n",
      " Wicks \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "   \n",
      " , in the Dwelling House of the said Job Wicks\n",
      "   , on the  8th of April  \n",
      "   last. It appeared that the Prisoner was the Prosecutor's  Servant  \n",
      "  ; and his fellow Servant deposed that he perceiving his Pocket to stick out, searched him, and found a Paper of Tobacco in it worth about 18 d. which he owned to him was his Master's, and that he took it out of his Cellar. The Constable also deposed that he confest he had taken Tobacco several times. The said Paper of Tobacco was produced in Court. The Jury considering the matter found him  \n",
      "  \n",
      "  \n",
      " Guilty to the value of 10 d.\n",
      "   \n",
      "  \n",
      "  \n",
      "  \n",
      " Transportation\n",
      "   . \n",
      " \n"
     ]
    }
   ],
   "source": [
    "# now let's get all text from first trial including text in child and grandchild tags\n",
    "# we will use .itertext() to iterate over all text in the div1 tag\n",
    "\n",
    "\n",
    "first_trial_text = \" \".join([text for text in trial_accounts[0].itertext()])\n",
    "print(\"First trial text:\", first_trial_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punishment: <Element 'rs' at 0x000001D3FC2F6CA0>\n",
      "Punishment tag: <rs id=\"t17200427-1-punish6\" type=\"punishmentDescription\">\n",
      " <interp inst=\"t17200427-1-punish6\" type=\"punishmentCategory\" value=\"transport\" />\n",
      " <join result=\"defendantPunishment\" targOrder=\"Y\" targets=\"t17200427-1-defend28 t17200427-1-punish6\" />\n",
      " Transportation\n",
      " </rs> .\n",
      "Punishment text: \n",
      "  \n",
      "  \n",
      " Transportation\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# let's try to get defendant Punishment if it exists\n",
    "# <rs id=\"t17200427-1-punish6\" type=\"punishmentDescription\">\n",
    "# we will use .find() to find the join tag\n",
    "# we will use .attrib to get the attributes of the join tag\n",
    "# we will use .get() to get the value of the attribute\n",
    "\n",
    "punishment = trial_accounts[0].find(\".//rs[@type='punishmentDescription']\")\n",
    "print(\"Punishment:\", punishment)\n",
    "# print whole punishment tag\n",
    "print(\"Punishment tag:\", ET.tostring(punishment, encoding=\"unicode\"))\n",
    "# iterate over all text in punishment tag\n",
    "print(\"Punishment text:\", \" \".join([text for text in punishment.itertext()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name OBC2-17200427.xml\n",
      "Year: 1720\n",
      "Year: 1720\n"
     ]
    }
   ],
   "source": [
    "# now year we will infer from the file name\n",
    "print(\"File name\", xml_files[0].name) # print file name\n",
    "# we will use .name to get the name of the file\n",
    "# we will use .split() to split the name of the file by _\n",
    "# we will use [1] to get the second part of the split\n",
    "# we will use .split() to split the second part of the split by -\n",
    "# we will use [0] to get the first part of the split\n",
    "# we will use int() to convert the year to integer\n",
    "\n",
    "year = int(xml_files[0].name.split(\"-\")[1][:4])\n",
    "print(\"Year:\", year)\n",
    "# we could have done this with regular expressions but this is simpler\n",
    "# regular expressions are very powerful but they are also very complicated\n",
    "# let's see regex version\n",
    "# we will use re.search() to search for a pattern in a string\n",
    "# we will use .group() to get the matched string\n",
    "# we will use \\d to match a digit\n",
    "# we will use {4} to match 4 digits\n",
    "\n",
    "year = int(re.search(r\"\\d{4}\", xml_files[0].name).group()) # we match on first 4 digits in the file name\n",
    "print(\"Year:\", year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial number: 1\n"
     ]
    }
   ],
   "source": [
    "# let's get trial number by n attribute from div1 tag\n",
    "# we will use .attrib to get the attributes of the div1 tag\n",
    "# we will use .get() to get the value of the attribute\n",
    "\n",
    "trial_number = trial_accounts[0].attrib[\"n\"]\n",
    "print(\"Trial number:\", trial_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First trial data: {'year': 1720, 'trial_number': '1', 'punishment': 'Transportation', 'text': \",of St. Leonard Eastcheap      , was indicted for     feloniously stealing 70 Pound weight of Tobacco, value 4 l. 10 s. the Goods of   Job  Wicks          , in the Dwelling House of the said Job Wicks  , on the 8th of April   last. It appeared that the Prisoner was the Prosecutor's Servant   ; and his fellow Servant deposed that he perceiving his Pocket to stick out, searched him, and found a Paper of Tobacco in it worth about 18 d. which he owned to him was his Master's, and that he took it out of his Cellar. The Constable also deposed that he confest he had taken Tobacco several times. The said Paper of Tobacco was produced in Court. The Jury considering the matter found him     Guilty to the value of 10 d.        Transportation  .\"}\n"
     ]
    }
   ],
   "source": [
    "# now that we know how to extract all the data we want from one trial let's extract all the data we want from all trials\n",
    "# let's create a function for each data piece we want to extract , function will take div1 tag as input and return data piece\n",
    "def get_year(file_name):\n",
    "    # if path object is Path we use .name to get the name of the file\n",
    "    if isinstance(file_name, Path):\n",
    "        file_name = file_name.name\n",
    "    return int(file_name.split(\"-\")[1][:4])\n",
    "\n",
    "def get_trial_number(div1):\n",
    "    # return div1.attrib[\"n\"] # this makes assumption that div1 tag has n attribute\n",
    "    # instead safer would be to use .get() method\n",
    "    return div1.attrib.get(\"n\", \"0\") # if n attribute does not exist return empty string\n",
    "\n",
    "\n",
    "def get_punishment(div1):\n",
    "    punishment = div1.find(\".//rs[@type='punishmentDescription']\")\n",
    "    if punishment is not None: # check if punishment exists\n",
    "        return \" \".join([text for text in punishment.itertext()])\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "def get_text(div1):\n",
    "    return \" \".join([text for text in div1.itertext()])\n",
    "\n",
    "def clean_text(text):\n",
    "    # first preprocess text a bit\n",
    "    return text.strip().replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"\\r\", \" \").replace(\"  \", \" \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# now let's create a function that will extract all the data we want from one trial\n",
    "def extract_trial_data(div1, file_name):\n",
    "    return {\n",
    "        \"year\": get_year(file_name),\n",
    "        \"trial_number\": get_trial_number(div1),\n",
    "        \"punishment\": clean_text(get_punishment(div1)),\n",
    "        \"text\": clean_text(get_text(div1))\n",
    "    }\n",
    "    # we are returning a dictionary with all the data we want\n",
    "\n",
    "\n",
    "\n",
    "# let's test on first trial\n",
    "print(\"First trial data:\", extract_trial_data(trial_accounts[0], xml_files[0].name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalizing text extraction to all elements\n",
    "\n",
    "If we can extract text from one element, we can extract text from all elements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trials: 75\n",
      "First 5 trials:\n",
      "{'year': 1720, 'trial_number': '1', 'punishment': 'Transportation', 'text': \",of St. Leonard Eastcheap      , was indicted for     feloniously stealing 70 Pound weight of Tobacco, value 4 l. 10 s. the Goods of   Job  Wicks          , in the Dwelling House of the said Job Wicks  , on the 8th of April   last. It appeared that the Prisoner was the Prosecutor's Servant   ; and his fellow Servant deposed that he perceiving his Pocket to stick out, searched him, and found a Paper of Tobacco in it worth about 18 d. which he owned to him was his Master's, and that he took it out of his Cellar. The Constable also deposed that he confest he had taken Tobacco several times. The said Paper of Tobacco was produced in Court. The Jury considering the matter found him     Guilty to the value of 10 d.        Transportation  .\"}\n",
      "{'year': 1720, 'trial_number': '2', 'punishment': 'Transportation', 'text': \"Alice  Jones         , of St. Michael's Cornhill      , was indicted for     privately stealing a Bermundas Hat, value 10 s. out of the Shop of    Edward  Hillior              , on the 21st of April   last. The Prosecutor's Servant deposed that the Prisner came into his Master's Shop and ask'd for a Hat of about 10 s. price; that he shewed several, and at last they agreed for one; but she said it was to go into the Country, and that she would stop into Bishopsgate-street. and if the Coach was not gone she would come and fetch it; that she went out of the Shop but he perceiving she could hardly walk fetcht her back again, and the Hat mentioned in the Indictment fell from between her Legs. Another deposed that he saw the former Evidence take the Hat from under her Petticoats. The Prisoner denyed the Fact, and called two Persons to her Reputation, who gave her a good Character, and said that she rented a House of 10 l. a Year in Petty France, at Westminster, but she had told the Justice that she liv'd in King-Street. The Jury considering the whole matter, found her     Guilty to the value of 10 d.        Transportation  .\"}\n",
      "{'year': 1720, 'trial_number': '3', 'punishment': 'Transportation', 'text': 'James  Wilson         , of St Katharine Coleman      , was indicted for     feloniously stealing 12 pound of Beef, value 4 s.  the Goods of    Charles  Watts           on the 9th of April   .   Mary  Watts        deposed that a Gentleman came up to her Bar about 8 a clock at Night, and told here there was a Fellow lurking about her House; that between 10 and 11 the Prisoner came to the Bar and took a Rump of Beef which hung there, but being pursued dropt it at the Door, that she saw him plain, and was sure the Prisoner was the same Person. The Cook-maid also deposed that she saw the Prisoner take the Beef, and that he lifted it up three times before be could get it off the Hook. The Prisoner in his Defence said, that he had been drinking, and ran his Head against the Beef, and it fell; but it appeared by the Evidence that he dropt it 10 yards from the Place where it hung. The Jury found him     Guilty to the value of 10 d.        Transportation  .'}\n",
      "{'year': 1720, 'trial_number': '4', 'punishment': 'Transportation', 'text': \"James  Mercy , alias   Masse          , of St. Andrew Undershaft      ; was indicted for     feloniously stealing 3 Silk Gowns and Petticoats, a Camblet Cloak, and other Goods, in the Dwelling House of    Mary  Robinson              , on the 15th of April   . The Prosecutor deposed, that she hearing a Noise above Stairs, sent the Maid up to see what it was.   Mary  Gough        deposed that she went up and found the Prisoner there with his Arms full of Clothes, whereupon she lock'd him in, ran down and call'd Assistance; that when they went up again the Clothes were found on the wet Floor, which before were lockt up in the Drawers. The Constable deposed that the Prisoner shamm'd being Drunk; and that he found a Chissel in his Pocket, with which he believes he open'd the Drawers. The Jury found him     Guilty to the value of 39 s.        Transportation  .\"}\n",
      "{'year': 1720, 'trial_number': '5', 'punishment': 'Transportation', 'text': 'Benjamin  Cook , alias   Richard Smith          , of St. Mary Abchurch      , was indicted for     feloniously stealing a Silver Cup, value 15 s.  the Goods of    Philip  Austin           , on the   29th of March   last. The Prosecutor deposed that he lost his Cup out of his Kitchen, which was produced in Court, and sworn to by him. A Goldsmith deposed that the Prisoner offered to sell it him, whereupon he stopt him and sent for Mr. Austin, who owned it. The Jury found him     Guilty to the value of 10 d.        Transportation  .'}\n"
     ]
    }
   ],
   "source": [
    "def get_list_of_trial_data(div1_list, file_name):\n",
    "    return [extract_trial_data(div1, file_name) for div1 in div1_list]\n",
    "\n",
    "# let's get all trial data in a list of dictionaries\n",
    "all_trial_data = get_list_of_trial_data(trial_accounts, xml_files[0].name)\n",
    "print(\"Number of trials:\", len(all_trial_data))\n",
    "# print first 5 trials\n",
    "print(\"First 5 trials:\", *all_trial_data[:5], sep=\"\\n\") # i unrolled the first 5 trials with * and separated them with new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trials: 75\n",
      "First 5 trials:\n",
      "{'year': 1720, 'trial_number': '1', 'punishment': 'Transportation', 'text': \",of St. Leonard Eastcheap      , was indicted for     feloniously stealing 70 Pound weight of Tobacco, value 4 l. 10 s. the Goods of   Job  Wicks          , in the Dwelling House of the said Job Wicks  , on the 8th of April   last. It appeared that the Prisoner was the Prosecutor's Servant   ; and his fellow Servant deposed that he perceiving his Pocket to stick out, searched him, and found a Paper of Tobacco in it worth about 18 d. which he owned to him was his Master's, and that he took it out of his Cellar. The Constable also deposed that he confest he had taken Tobacco several times. The said Paper of Tobacco was produced in Court. The Jury considering the matter found him     Guilty to the value of 10 d.        Transportation  .\"}\n",
      "{'year': 1720, 'trial_number': '2', 'punishment': 'Transportation', 'text': \"Alice  Jones         , of St. Michael's Cornhill      , was indicted for     privately stealing a Bermundas Hat, value 10 s. out of the Shop of    Edward  Hillior              , on the 21st of April   last. The Prosecutor's Servant deposed that the Prisner came into his Master's Shop and ask'd for a Hat of about 10 s. price; that he shewed several, and at last they agreed for one; but she said it was to go into the Country, and that she would stop into Bishopsgate-street. and if the Coach was not gone she would come and fetch it; that she went out of the Shop but he perceiving she could hardly walk fetcht her back again, and the Hat mentioned in the Indictment fell from between her Legs. Another deposed that he saw the former Evidence take the Hat from under her Petticoats. The Prisoner denyed the Fact, and called two Persons to her Reputation, who gave her a good Character, and said that she rented a House of 10 l. a Year in Petty France, at Westminster, but she had told the Justice that she liv'd in King-Street. The Jury considering the whole matter, found her     Guilty to the value of 10 d.        Transportation  .\"}\n",
      "{'year': 1720, 'trial_number': '3', 'punishment': 'Transportation', 'text': 'James  Wilson         , of St Katharine Coleman      , was indicted for     feloniously stealing 12 pound of Beef, value 4 s.  the Goods of    Charles  Watts           on the 9th of April   .   Mary  Watts        deposed that a Gentleman came up to her Bar about 8 a clock at Night, and told here there was a Fellow lurking about her House; that between 10 and 11 the Prisoner came to the Bar and took a Rump of Beef which hung there, but being pursued dropt it at the Door, that she saw him plain, and was sure the Prisoner was the same Person. The Cook-maid also deposed that she saw the Prisoner take the Beef, and that he lifted it up three times before be could get it off the Hook. The Prisoner in his Defence said, that he had been drinking, and ran his Head against the Beef, and it fell; but it appeared by the Evidence that he dropt it 10 yards from the Place where it hung. The Jury found him     Guilty to the value of 10 d.        Transportation  .'}\n",
      "{'year': 1720, 'trial_number': '4', 'punishment': 'Transportation', 'text': \"James  Mercy , alias   Masse          , of St. Andrew Undershaft      ; was indicted for     feloniously stealing 3 Silk Gowns and Petticoats, a Camblet Cloak, and other Goods, in the Dwelling House of    Mary  Robinson              , on the 15th of April   . The Prosecutor deposed, that she hearing a Noise above Stairs, sent the Maid up to see what it was.   Mary  Gough        deposed that she went up and found the Prisoner there with his Arms full of Clothes, whereupon she lock'd him in, ran down and call'd Assistance; that when they went up again the Clothes were found on the wet Floor, which before were lockt up in the Drawers. The Constable deposed that the Prisoner shamm'd being Drunk; and that he found a Chissel in his Pocket, with which he believes he open'd the Drawers. The Jury found him     Guilty to the value of 39 s.        Transportation  .\"}\n",
      "{'year': 1720, 'trial_number': '5', 'punishment': 'Transportation', 'text': 'Benjamin  Cook , alias   Richard Smith          , of St. Mary Abchurch      , was indicted for     feloniously stealing a Silver Cup, value 15 s.  the Goods of    Philip  Austin           , on the   29th of March   last. The Prosecutor deposed that he lost his Cup out of his Kitchen, which was produced in Court, and sworn to by him. A Goldsmith deposed that the Prisoner offered to sell it him, whereupon he stopt him and sent for Mr. Austin, who owned it. The Jury found him     Guilty to the value of 10 d.        Transportation  .'}\n"
     ]
    }
   ],
   "source": [
    "# now let's make a function that takes file name and returns all trial data\n",
    "def get_all_trial_data(file_name):\n",
    "    # first we will parse the xml file\n",
    "    tree = ET.parse(file_name)\n",
    "    root = tree.getroot()\n",
    "    # then we will get all trialAccount div1 tags\n",
    "    trial_accounts = [div1 for div1 in root.findall(\".//div1[@type='trialAccount']\")]\n",
    "    # then we will get all trial data\n",
    "    return get_list_of_trial_data(trial_accounts, file_name)\n",
    "\n",
    "# let's test on first file\n",
    "also_all_trial_data = get_all_trial_data(xml_files[0])\n",
    "print(\"Number of trials:\", len(also_all_trial_data))\n",
    "# print first 5 trials\n",
    "print(\"First 5 trials:\", *also_all_trial_data[:5], sep=\"\\n\") # i unrolled the first 5 trials with * and separated them with new line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over all files in a folder\n",
    "\n",
    "If we can extract data from one file, we can extract data from all files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My list: [1, 2, 3, [4, 5, 6]]\n",
      "My list: [1, 2, 3, [4, 5, 6], 4, 5, 6]\n",
      "My list: [1, 2, 3, [4, 5, 6], 4, 5, 6, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# why use extend instead of append\n",
    "# append adds one element to the list\n",
    "my_list = [1,2,3]\n",
    "another_list = [4,5,6]\n",
    "my_list.append(another_list)\n",
    "print(\"My list:\", my_list)\n",
    "# if i want flat list i have to do this\n",
    "my_list.extend(another_list)\n",
    "print(\"My list:\", my_list)\n",
    "# alternative syntax would +=\n",
    "my_list += another_list # works similarly to extend\n",
    "print(\"My list:\", my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 23.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trials: 1637\n",
      "Random 5 trials:\n",
      "{'year': 1872, 'trial_number': '69', 'punishment': 'DEATH', 'text': '185.         CHRISTIANA EDMUNDS (34) , was indicted    for the wilful murder of       Sidney Albert Barker .      MR. SERJEANT BALLANTINE , with MR. STRAIGHT , conducted the Prosecution; and      MR. SERJEANT PARRY , with MR. WPBSLEY , the Defence.        GUILTY   . —       DEATH   .    ADJOURNED TO MONDAY, JANUARY 29 TH , 1872.'}\n",
      "{'year': 1742, 'trial_number': '76', 'punishment': '[Transportation. See summary.]', 'text': '88.    John  Charlton         , was indicted for     entering the House of   William  Hide          , and stealing from thence two Coats and other Things  , April 28   .      Guilty  .         [Transportation. See summary.]'}\n",
      "{'year': 1720, 'trial_number': '52', 'punishment': 'Transportation', 'text': \"Elizabeth  Walters         of St. Paul's Covent-Garden      , was indicted for     feloniously stealing a Poplin Gown and Petticoat, &c.  the Goods of    Sarah  Curtis           . It appeared that the Prisoner came to the Prosecutor, desiring that she might be there till she could provide her self a Lodging, took the Goods and went away, but was found in Holbourn with the Clothes on, which she had wore ever since, and had on at her Tryal. The Jury found her     Guilty to the value of 10 d.        Transportation  .\"}\n",
      "{'year': 1902, 'trial_number': '75', 'punishment': '', 'text': '640.       THOMAS REIGAN , Robbery      with violence, with two other persons unknown, on       William Ellis , and stealing 30s., his money.      MR. COHEN   Prosecuted , and MR. STEPHENSON   Defended.            WILLIAM ELLIS   . I am a butcher, of 21, Devonshire Road, Holloway — on August 15th, about 11 p.m., I was going through Mill\\'s Court, Shore-ditch, eating fish and potatoes — a woman came up and asked me to give her some, and I told-her to take the lot — suddenly I was seized from behind, both my arms were pinned down, and two pairs of hands held my throat, nearly strangling me — the prisoner came in front of me and took all the money I had, about 30s., from my pockets — they all then ran away towards High Street, and I followed — I did not lose sight of them until a constable caught the prisoner, and I gave him in charge.    Cross-examined.  The court was rather dark — I had not seen the woman before — I am quite sure it was the prisoner who rifled my pockets — I did not hear him call out, \"Stop thief,\" neither did he say to me, \"What\\'s up?\" — I know 30s. was found on him when he was searched.          WILLIAM PECK   (245 G. ) I saw the prisoner with two others rush out of Mill\\'s Court — I caught him and asked him what he was running for — he said, \"I couldn\\'t help it, I had to\" — the prosecutor came up and said to me, \"You have got the right man; that is the man who put his hand in my trousers pocket and took my money while the other two nearly strangled me\" — I took him to the station and searched him, and found 34s. on him — he said that was his and his son\\'s wages received that night — when the prosecutor came up the prisoner said, \"I was running after the others\" — when I first saw the prisoner he was about two yards in front of the others — he certainly was not running after them nor calling out against them.    Cross-examined.  He did seem confused when he saw me running behind him, and stopped — I said so at the Police Court, but the evidence was cut short, I was told simply to state the charge — I ascertained that the prisoner was paid 30s. that day, also that he bears a good character and was in the Army twenty-one years.    The prisoner, in his defence on oath, said that he was returning home through Mill\\'s Court, and he met a woman who said to him, \"What are they going to do with this man?\"; that the prosecutor was held by two men, one in front and one behind; that he said, \"Hallo\"; that they ran off and he ran after them, and was stopped by the constable and charged by the prosecutor, and denied ever having touched him. He received a good character.          NOT GUILTY   .'}\n",
      "{'year': 1814, 'trial_number': '11', 'punishment': '', 'text': \"82.    WILLIAM  ROCHESTER         was indicted for     feloniously stealing, on the 8th of January   , two stockings, value 18 d. the property of    Henry  Dickenson         ; one iron pot, value 5 s. and a feather bed, value 4 s.  the property of    John  Powell         .      HENRY  DICKENSON        . I live in Palmer's-green, in the parish of Edmonton . In the month of August last, my drying ground was robbed of sundry articles that were left there after a wash.  Q. Among other things did you lose any stockings - A. I did, two stockings. I have seen them since in the possession of the patrol.  Q. Had you seen them yourself in the drying ground - A. No, the servant who had the charge of them at that time is out of my service. I cannot tell when last I had them in my possession, nor how they went out of my possession.      JOHN  POWELL        . I lost an iron pot and a feather bed the latter end of July last. I lost them from a shed in my yard. I saw them the day before I missed them, and I saw them again last Saturday night in the possession of Smith the patrol.  Q. Do you know the prisoner - A. Yes, he lived near me. I can speak to the iron pot being my property. The prisoner took the pot, and left the lid at home. I know the pot by the lid fitting it. This bed I know by the stripe of the tick; it is large blue stripes.      WILLIAM  SMITH        . I am a patrol. I got this iron pot and feather bed out of Rochester's house on Saturday evening last. I found the feather bed enclosed in a blanket to make a pillow of; it was on the bed on which Rochester slept. I found the pot in the same house, but not in the same room; it was in Rochester's part of the house. I asked the prisoner where he got the feather bed; he said it was a bed that his wife's father gave him seven years ago. He said he found the pot in a ditch.  Prisoner's Defence. I found the pot. My wife's father gave her the bed when he went in the workhouse. He has lost the use of his limbs; he cannot get across the place without being carried.      NOT GUILTY  .  First Middlesex jury, before Mr. Justice Bailey.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# let's write a function that will take a list of file names and return all trial data\n",
    "def get_all_trial_data_from_list(file_names):\n",
    "    trial_data = []\n",
    "    for file_name in tqdm(file_names): # tqdm is optional for showing progress bar\n",
    "        trial_data.extend(get_all_trial_data(file_name)) # get_all_trial_data returns a list so we will extend our trial_data list with that list\n",
    "    return trial_data\n",
    "\n",
    "# let's test it on all xml files that we have\n",
    "all_trial_data = get_all_trial_data_from_list(xml_files)\n",
    "print(\"Number of trials:\", len(all_trial_data))\n",
    "# print random selection of 5 trials\n",
    "random.seed(42) # set random seed so we get the same random selection every time\n",
    "print(\"Random 5 trials:\", *random.sample(all_trial_data, 5), sep=\"\\n\") # i unrolled the first 5 trials with * and separated them with new line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving lists of dictionaries to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (1637, 4)\n",
      "Dataframe columns: Index(['year', 'trial_number', 'punishment', 'text'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>trial_number</th>\n",
       "      <th>punishment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1720</td>\n",
       "      <td>1</td>\n",
       "      <td>Transportation</td>\n",
       "      <td>,of St. Leonard Eastcheap      , was indicted ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1720</td>\n",
       "      <td>2</td>\n",
       "      <td>Transportation</td>\n",
       "      <td>Alice  Jones         , of St. Michael's Cornhi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1720</td>\n",
       "      <td>3</td>\n",
       "      <td>Transportation</td>\n",
       "      <td>James  Wilson         , of St Katharine Colema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1720</td>\n",
       "      <td>4</td>\n",
       "      <td>Transportation</td>\n",
       "      <td>James  Mercy , alias   Masse          , of St....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1720</td>\n",
       "      <td>5</td>\n",
       "      <td>Transportation</td>\n",
       "      <td>Benjamin  Cook , alias   Richard Smith        ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year trial_number      punishment  \\\n",
       "0  1720            1  Transportation   \n",
       "1  1720            2  Transportation   \n",
       "2  1720            3  Transportation   \n",
       "3  1720            4  Transportation   \n",
       "4  1720            5  Transportation   \n",
       "\n",
       "                                                text  \n",
       "0  ,of St. Leonard Eastcheap      , was indicted ...  \n",
       "1  Alice  Jones         , of St. Michael's Cornhi...  \n",
       "2  James  Wilson         , of St Katharine Colema...  \n",
       "3  James  Mercy , alias   Masse          , of St....  \n",
       "4  Benjamin  Cook , alias   Richard Smith        ...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lists of dictionaries are very easy to save in pandas as dataframe table\n",
    "# keys of dictionaries will be columns of the dataframe\n",
    "# values of dictionaries will be rows of the dataframe\n",
    "df = pd.DataFrame(all_trial_data)\n",
    "print(\"Dataframe shape:\", df.shape)\n",
    "print(\"Dataframe columns:\", df.columns)\n",
    "# first 5 rows of dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV path: d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\old_bailey_sample_1720_1913.csv\n",
      "Parquet path: d:\\Github\\BSSDH_2023_workshop\\notebooks\\temp\\old_bailey_sample_1720_1913.parquet\n"
     ]
    }
   ],
   "source": [
    "# now we want to save the results to csv file and possibly to parquet file\n",
    "# parquet files preserve data types and are more efficient than csv files\n",
    "# we will use pandas to save the data to csv and parquet files\n",
    "# we will use .to_csv() to save the data to csv file\n",
    "# we will use .to_parquet() to save the data to parquet file\n",
    "\n",
    "file_stem = \"old_bailey_sample_1720_1913\"\n",
    "\n",
    "# we will use Path() to create a path to our file\n",
    "csv_path = Path(Path.cwd() / \"temp\" / (file_stem + \".csv\"))\n",
    "print(\"CSV path:\", csv_path)\n",
    "# save to csv file\n",
    "df.to_csv(csv_path, index=False) # we will use index=False to not save the index column\n",
    "\n",
    "parquet_path = Path(Path.cwd() / \"temp\" / (file_stem + \".parquet\"))\n",
    "print(\"Parquet path:\", parquet_path)\n",
    "# save to parquet file\n",
    "df.to_parquet(parquet_path, index=False) # we will use index=False to not save the index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we might need to more cleaning of data later\n",
    "\n",
    "# we would want to do things like\n",
    "# remove extra whitespace\n",
    "# we will use .strip() to strip the text of the div1 tag of leading and trailing whitespace\n",
    "# we will use .replace(\"\\n\", \" \") to replace new line characters with space\n",
    "# we will use .replace(\"\\t\", \" \") to replace tab characters with space\n",
    "# we will use .replace(\"\\r\", \" \") to replace carriage return characters with space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra discussion - reading other formats\n",
    "\n",
    "Besides XML that we just downloaded, read, parsed and deconstructed there are many other file formats that contain text data. \n",
    "Some of the most common are:\n",
    "\n",
    "* PDF - Portable Document Format - very common format for documents - can be text based or image based\n",
    "* DOCX - Microsoft Word format - very common format for documents - can be text based or image based\n",
    "* HTML - HyperText Markup Language - very common format for web pages - can be text based or image based\n",
    "* TXT - Text file - very common format for text data - can be text based or image based\n",
    "* CSV - Comma Separated Values - very common format for tabular data - can be text based or image based\n",
    "* JSON - JavaScript Object Notation - very common format for data exchange - can be text based or image based\n",
    "* XML - eXtensible Markup Language - very common format for data exchange - what we just used\n",
    "\n",
    "### Questions to class\n",
    "\n",
    "1. What is your favorite file format?\n",
    "2. What is your least favorite file format?\n",
    "3. What is the most common file format you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Time allowing we can look into reading some of the other data in the xml file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
